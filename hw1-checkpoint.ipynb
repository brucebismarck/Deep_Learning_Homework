{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 790.139: Natural Language Processing (Fall 2017): Coding Homework 1 (Word Embedding Training, Visualization, Evaluation)\n",
    "Created by TA Yixin Nie (Instructor: Mohit Bansal)\n",
    "\n",
    "\n",
    "# Instructions\n",
    "\n",
    "<strong>All the instructions are present in the jupyter notebook (as shown in the class; and see an html preview below).  \n",
    "Install jupyter notebook in your python environment and download the file below.  \n",
    "https://drive.google.com/drive/folders/0B6i0pVGwapCdaU9KYmFZNEFGZmM?usp=sharing  \n",
    "  \n",
    "  \n",
    "Use this directory as your workspace and write your code in the “hw1.ipynb” file. You could also add extra images or tables in the directory and link them into “hw1.ipynb” file but grading will only based on the “hw1.ipynb” file.  \n",
    "\n",
    "Name your directory as `\"<your_name>_hw1\"` and compress it to `\"<your_name>_hw1.zip\"`.   \n",
    "Email the file to <a href=\"mailto:comp790.hw@gmail.com\">comp790.hw@gmail.com</a> for submission.<strong>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Homework 1 Preview\n",
    "The main goals for homework 1 are:\n",
    "1. Setting up your python environment.\n",
    "2. Getting familiar with the most popular deep learning framework (Pytorch, Tensorflow).\n",
    "3. Starting your very first NLP project by training a toy word embedding from scratch.\n",
    "4. Visualizing your word vectors.\n",
    "5. Trying to learn to interpret your results.\n",
    "\n",
    "Notice:  \n",
    "You can test or run your code in any environment but you could only show your codes, your results and your write-ups in this single notebook file. We will not re-run your code for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup python environment (1 pt)\n",
    "We will use python3.x throughout this course and make sure to have the packages (listed below) installed in your python environment.  \n",
    "Required packages:\n",
    "```\n",
    "numpy\n",
    "ipython\n",
    "jupyter\n",
    "```\n",
    "Recommended packages:\n",
    "```\n",
    "torch\n",
    "torchvision \n",
    "tensorflow\n",
    "tqdm\n",
    "scikit-learn\n",
    "matplotlib\n",
    "nltk\n",
    "spacy\n",
    "```\n",
    "You can use pytorch or tensorflow for building your neural network model. However, I highly recommend using pytorch as your default deep learning toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data (0.5 pt)\n",
    "Successful opening this file means that you have set up your jupyter notebook and ready to code. Now that the first thing you need to do is to load the dataset. The dataset is in the file \"dataset.txt\" and it contains almost 19k sentences. Loading all the sentences into an array (one sentence per item) and print out the total length. Write and run your code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98307\n"
     ]
    }
   ],
   "source": [
    "text_file = open(\"dataset.txt\", \"r\")\n",
    "lines_list = text_file.read().split('\\n')\n",
    "lines_array = np.asarray(lines_list)\n",
    "print(len(lines_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the professors helped the students'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "### (1). Tokenization (0.5 pt)\n",
    "For many NLP tasks, the first thing you need to do is to tokenize your raw text into lists of words. You can use `spacy` or `nltk` to tokenize the sentences or you can just use `split(\" \")` to break the sentences into a list of words.  \n",
    "Write and run your code in the next cell to tokenize all the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n",
      "the professors didn't help the students\n",
      "['the', 'professors', 'did', \"n't\", 'help', 'the', 'students']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "tokenization = list()\n",
    "for item in lines_list:\n",
    "    doc = nlp(item)\n",
    "    temp_list = list()\n",
    "    for token in doc:\n",
    "        temp_list.append(token.orth_) # keep string rather than token\n",
    "    tokenization.append(temp_list)\n",
    "print(lines_list[1])\n",
    "print(tokenization[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2). Showing word statistics (0.5 pt)\n",
    "Using `Counter` in python to calculate the frequency for each word in the dataset.  \n",
    "Show the frequency for words in `['man', 'woman', 'is', 'was']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of word \"man\" is 1282\n",
      "Frequency of word \"woman\" is 918\n",
      "Frequency of word \"is\" is 21982\n",
      "Frequency of word \"was\" is 18330\n"
     ]
    }
   ],
   "source": [
    "word_list = ['man', 'woman', 'is', 'was']\n",
    "import collections\n",
    "flat_list = [item for sublist in tokenization for item in sublist]\n",
    "\n",
    "total_counter = collections.Counter(flat_list)\n",
    "for item in word_list:\n",
    "    print('Frequency of word \"' + item + '\" is ' + str(total_counter[item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3). building vocabulary (0.5)\n",
    "Select the top `K` most frequent word in the dataset and build a vocabulary for those words. (You can set `K=1000` for now.)  \n",
    "Building a vocabulary is nothing more than assigning a unique id to each word in the dataset. So, a vocabulary is basically a dictionary and an array in python data structure. The dictionary will convert the word to a number and the array will convert a given number to a word.\n",
    "```python\n",
    "# e.g.\n",
    "stoi['love'] = 520\n",
    "itos[520] = 'love'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "professors\n"
     ]
    }
   ],
   "source": [
    "top_words_list = list(dict(total_counter).keys())[0:1000]\n",
    "num_list = range(0,1000)\n",
    "\n",
    "stoi = dict(zip(top_words_list,num_list))\n",
    "itos = dict(zip(num_list,top_words_list))\n",
    "\n",
    "print(stoi['professors'])\n",
    "print(itos[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Rearranging (5 pt)\n",
    "Read http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ and then rearrange your data.\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/training_data.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "The final structure of your data should be a long list of tuples (`x`, `y`).  \n",
    "`x` is the id of the target word (the center word in current window) and `y` is the id of the context word.\n",
    "\n",
    "Write your code in the cell below. Print out the total length of all your samples.\n",
    "\n",
    "Notice:\n",
    "For now, you can just ignore the words that are not in the set of the top `K` most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words = list(stoi.keys())\n",
    "k = 5\n",
    "if k % 2 == 1 and k > 0:\n",
    "    half_k = (k-1)//2\n",
    "else:\n",
    "    print ('k need to be a positive odd number')\n",
    "\n",
    "rearrange_list = list()\n",
    "\n",
    "for line in tokenization:\n",
    "    for item in top_words:\n",
    "        if item in line:\n",
    "            line_len = len(line)\n",
    "            loc = line.index(item)\n",
    "            near_range = list(range(loc - half_k, loc+ half_k+1))\n",
    "            for pos in near_range:\n",
    "                if pos >= 0 and pos <= line_len -1 and pos != loc:\n",
    "                    if line[pos] in top_words:\n",
    "                        rearrange_list.append(tuple([stoi[item], stoi[line[pos]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263454\n"
     ]
    }
   ],
   "source": [
    "print(len(rearrange_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build word embedding (skip-gram) model. (15 pt)\n",
    "It's time to build your very first NLP neural network model, the skip-gram model!  \n",
    "You should first read tutorials for pytorch http://pytorch.org/tutorials/ or tensorflow if you are not familiar with the neural network frameworks and models.  \n",
    "\n",
    "Some key concepts you need to know:\n",
    "\n",
    "* Softmax and cross entropy loss.\n",
    "* Batching. (esp. why do we need batch and how?)\n",
    "* Forward and backward propagation.  \n",
    "\n",
    "Keep it in mind that the model will have two trainable matrices, namely the word embedding matrix and the context embedding matrix.  \n",
    "\n",
    "Write your code in the cell below and print out the shape of all the parameters in your model.  \n",
    "(It should be something like [vocabulary_size * word_embedding_dimension].)\n",
    "\n",
    "Read http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model carefully again before building your neural network.\n",
    "\n",
    "<img src=\"http://ruder.io/content/images/2016/06/softmax_classifier.png\" alt=\"\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_list = [item[0] for item in rearrange_list]\n",
    "context_list = [item[1] for item in rearrange_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 128 #(batch size)\n",
    "EMBEDDING_DIM = 300\n",
    "vocab_size = 1000\n",
    "\n",
    "vocab = top_words_list\n",
    "word_to_ix = stoi\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class SkipgramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(SkipgramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "   \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = self.linear1(embeds)\n",
    "        probs = F.softmax(out)\n",
    "        return probs\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "model = SkipgramModel(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e+05 *\n",
      "  1.2006\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      "  1.1988\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      "  1.1984\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      "  1.1981\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,5):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for t in range((len(input_list)//CONTEXT_SIZE)):\n",
    "        sublist = rearrange_list[t*CONTEXT_SIZE: (t+1)*CONTEXT_SIZE]\n",
    "        context = list(map(operator.itemgetter(0),sublist))\n",
    "        target =  list(map(operator.itemgetter(1),sublist))\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        #context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = Variable(torch.LongTensor(context))\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        probs = model(context_var)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(probs, Variable(torch.LongTensor(target)))\n",
    "    \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "    print(total_loss)\n",
    "#    losses.append(total_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the neural model.\n",
    "### (1). Converting data type and batching. (5 pt)\n",
    "Before training your neural model, you should first convert the type of your data to acceptable input type of your neural framework.  \n",
    "For example, if you are using pytorch, the type of your input tuple is `(int, int)`. You need to convert it into `(torch.LongTensor, torch.LongTensor)` such that it can be accepted by your neural model.\n",
    "\n",
    "You will also need to manually batch your data. For simplicity, you can just build a big matrix with dimension `(number_of_examples, 2)` and then select data batch by batch.\n",
    "\n",
    "There is a function call (`batch_index_gen`) that might be helpful.\n",
    "\n",
    "Write your code in the cell below. Print out the type of your input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8), (8, 16), (16, 24), (24, 32), (32, 40), (40, 48), (48, 50)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_index_gen(batch_size, size):\n",
    "    batch_indexer = []\n",
    "    start = 0\n",
    "    while start < size:\n",
    "        end = start + batch_size\n",
    "        if end > size:\n",
    "            end = size\n",
    "        batch_indexer.append((start, end))\n",
    "        start = end\n",
    "    return batch_indexer\n",
    "\n",
    "batch_index_gen(8, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (1, 0),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 0),\n",
       " (2, 3),\n",
       " (3, 2),\n",
       " (3, 0),\n",
       " (0, 1),\n",
       " (0, 4),\n",
       " (1, 0),\n",
       " (1, 4),\n",
       " (1, 5),\n",
       " (3, 6),\n",
       " (3, 0),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 5),\n",
       " (4, 6),\n",
       " (5, 1),\n",
       " (5, 4),\n",
       " (5, 6),\n",
       " (5, 0),\n",
       " (6, 4),\n",
       " (6, 5),\n",
       " (6, 0),\n",
       " (6, 3),\n",
       " (0, 1),\n",
       " (0, 6),\n",
       " (1, 0),\n",
       " (1, 6),\n",
       " (1, 0),\n",
       " (6, 0),\n",
       " (6, 1),\n",
       " (6, 0),\n",
       " (6, 7),\n",
       " (7, 6),\n",
       " (7, 0),\n",
       " (0, 1),\n",
       " (0, 8),\n",
       " (1, 0),\n",
       " (1, 8),\n",
       " (1, 9),\n",
       " (3, 6),\n",
       " (3, 0),\n",
       " (6, 8),\n",
       " (6, 9),\n",
       " (6, 0),\n",
       " (6, 3),\n",
       " (8, 0),\n",
       " (8, 1),\n",
       " (8, 9),\n",
       " (8, 6),\n",
       " (9, 1),\n",
       " (9, 8),\n",
       " (9, 6),\n",
       " (9, 0),\n",
       " (0, 1),\n",
       " (0, 10),\n",
       " (1, 0),\n",
       " (1, 10),\n",
       " (1, 11),\n",
       " (3, 11),\n",
       " (3, 0),\n",
       " (10, 0),\n",
       " (10, 1),\n",
       " (10, 11),\n",
       " (10, 0),\n",
       " (11, 1),\n",
       " (11, 10),\n",
       " (11, 0),\n",
       " (11, 3),\n",
       " (0, 12),\n",
       " (0, 13),\n",
       " (5, 12),\n",
       " (5, 13),\n",
       " (5, 11),\n",
       " (5, 0),\n",
       " (7, 11),\n",
       " (7, 0),\n",
       " (11, 13),\n",
       " (11, 5),\n",
       " (11, 0),\n",
       " (11, 7),\n",
       " (12, 0),\n",
       " (12, 13),\n",
       " (12, 5),\n",
       " (13, 0),\n",
       " (13, 12),\n",
       " (13, 5),\n",
       " (13, 11),\n",
       " (0, 1),\n",
       " (0, 14),\n",
       " (1, 0),\n",
       " (1, 14),\n",
       " (1, 11),\n",
       " (3, 11),\n",
       " (3, 0),\n",
       " (11, 1),\n",
       " (11, 14),\n",
       " (11, 0),\n",
       " (11, 3),\n",
       " (14, 0),\n",
       " (14, 1),\n",
       " (14, 11),\n",
       " (14, 0),\n",
       " (0, 12),\n",
       " (0, 15),\n",
       " (7, 11),\n",
       " (7, 0),\n",
       " (9, 12),\n",
       " (9, 15),\n",
       " (9, 11),\n",
       " (9, 0),\n",
       " (11, 15),\n",
       " (11, 9),\n",
       " (11, 0),\n",
       " (11, 7),\n",
       " (12, 0),\n",
       " (12, 15),\n",
       " (12, 9),\n",
       " (15, 0),\n",
       " (15, 12),\n",
       " (15, 9),\n",
       " (15, 11),\n",
       " (0, 7),\n",
       " (0, 13),\n",
       " (2, 7),\n",
       " (2, 13),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (7, 0),\n",
       " (7, 13),\n",
       " (7, 2),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (13, 0),\n",
       " (13, 7),\n",
       " (13, 2),\n",
       " (13, 16),\n",
       " (16, 13),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (0, 7),\n",
       " (0, 13),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 13),\n",
       " (2, 9),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (7, 0),\n",
       " (7, 13),\n",
       " (7, 9),\n",
       " (9, 7),\n",
       " (9, 13),\n",
       " (9, 2),\n",
       " (9, 16),\n",
       " (13, 0),\n",
       " (13, 7),\n",
       " (13, 9),\n",
       " (13, 2),\n",
       " (16, 9),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (0, 3),\n",
       " (0, 14),\n",
       " (2, 3),\n",
       " (2, 14),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (3, 0),\n",
       " (3, 14),\n",
       " (3, 2),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (14, 0),\n",
       " (14, 3),\n",
       " (14, 2),\n",
       " (14, 16),\n",
       " (16, 14),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (0, 3),\n",
       " (0, 14),\n",
       " (2, 14),\n",
       " (2, 9),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (3, 0),\n",
       " (3, 14),\n",
       " (3, 9),\n",
       " (9, 3),\n",
       " (9, 14),\n",
       " (9, 2),\n",
       " (9, 16),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (14, 0),\n",
       " (14, 3),\n",
       " (14, 9),\n",
       " (14, 2),\n",
       " (16, 9),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (0, 7),\n",
       " (0, 13),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 13),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (7, 0),\n",
       " (7, 13),\n",
       " (7, 17),\n",
       " (13, 0),\n",
       " (13, 7),\n",
       " (13, 17),\n",
       " (13, 2),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (17, 7),\n",
       " (17, 13),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (0, 7),\n",
       " (0, 13),\n",
       " (2, 9),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (7, 0),\n",
       " (7, 13),\n",
       " (7, 9),\n",
       " (9, 7),\n",
       " (9, 13),\n",
       " (9, 17),\n",
       " (9, 2),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (13, 0),\n",
       " (13, 7),\n",
       " (13, 9),\n",
       " (13, 17),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (17, 13),\n",
       " (17, 9),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (0, 3),\n",
       " (0, 14),\n",
       " (2, 14),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (3, 0),\n",
       " (3, 14),\n",
       " (3, 17),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (14, 0),\n",
       " (14, 3),\n",
       " (14, 17),\n",
       " (14, 2),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (17, 3),\n",
       " (17, 14),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (0, 7),\n",
       " (0, 15),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 9),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (7, 0),\n",
       " (7, 15),\n",
       " (7, 9),\n",
       " (9, 7),\n",
       " (9, 15),\n",
       " (9, 17),\n",
       " (9, 2),\n",
       " (15, 0),\n",
       " (15, 7),\n",
       " (15, 9),\n",
       " (15, 17),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (17, 15),\n",
       " (17, 9),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (1, 0),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 0),\n",
       " (2, 18),\n",
       " (18, 2),\n",
       " (18, 0),\n",
       " (0, 1),\n",
       " (0, 4),\n",
       " (1, 0),\n",
       " (1, 4),\n",
       " (1, 9),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 9),\n",
       " (4, 6),\n",
       " (6, 4),\n",
       " (6, 9),\n",
       " (6, 0),\n",
       " (6, 19),\n",
       " (9, 1),\n",
       " (9, 4),\n",
       " (9, 6),\n",
       " (9, 0),\n",
       " (19, 6),\n",
       " (19, 0),\n",
       " (0, 1),\n",
       " (0, 6),\n",
       " (1, 0),\n",
       " (1, 6),\n",
       " (1, 0),\n",
       " (6, 0),\n",
       " (6, 1),\n",
       " (6, 0),\n",
       " (6, 19),\n",
       " (19, 6),\n",
       " (19, 0),\n",
       " (0, 12),\n",
       " (0, 20),\n",
       " (6, 20),\n",
       " (6, 9),\n",
       " (6, 0),\n",
       " (6, 18),\n",
       " (9, 12),\n",
       " (9, 20),\n",
       " (9, 6),\n",
       " (9, 0),\n",
       " (12, 0),\n",
       " (12, 20),\n",
       " (12, 9),\n",
       " (18, 6),\n",
       " (18, 0),\n",
       " (20, 0),\n",
       " (20, 12),\n",
       " (20, 9),\n",
       " (20, 6),\n",
       " (0, 12),\n",
       " (0, 13),\n",
       " (11, 12),\n",
       " (11, 13),\n",
       " (11, 0),\n",
       " (11, 19),\n",
       " (12, 0),\n",
       " (12, 13),\n",
       " (12, 11),\n",
       " (13, 0),\n",
       " (13, 12),\n",
       " (13, 11),\n",
       " (13, 0),\n",
       " (19, 11),\n",
       " (19, 0),\n",
       " (0, 1),\n",
       " (0, 10),\n",
       " (1, 0),\n",
       " (1, 10),\n",
       " (1, 9),\n",
       " (9, 1),\n",
       " (9, 10),\n",
       " (9, 11),\n",
       " (9, 0),\n",
       " (10, 0),\n",
       " (10, 1),\n",
       " (10, 9),\n",
       " (10, 11),\n",
       " (11, 10),\n",
       " (11, 9),\n",
       " (11, 0),\n",
       " (11, 19),\n",
       " (19, 11),\n",
       " (19, 0),\n",
       " (0, 12),\n",
       " (0, 15),\n",
       " (11, 12),\n",
       " (11, 15),\n",
       " (11, 0),\n",
       " (11, 18),\n",
       " (12, 0),\n",
       " (12, 15),\n",
       " (12, 11),\n",
       " (15, 0),\n",
       " (15, 12),\n",
       " (15, 11),\n",
       " (15, 0),\n",
       " (18, 11),\n",
       " (18, 0),\n",
       " (0, 12),\n",
       " (0, 15),\n",
       " (9, 12),\n",
       " (9, 15),\n",
       " (9, 11),\n",
       " (9, 0),\n",
       " (11, 15),\n",
       " (11, 9),\n",
       " (11, 0),\n",
       " (11, 18),\n",
       " (12, 0),\n",
       " (12, 15),\n",
       " (12, 9),\n",
       " (15, 0),\n",
       " (15, 12),\n",
       " (15, 9),\n",
       " (15, 11),\n",
       " (18, 11),\n",
       " (18, 0),\n",
       " (0, 19),\n",
       " (0, 13),\n",
       " (2, 19),\n",
       " (2, 13),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (13, 0),\n",
       " (13, 19),\n",
       " (13, 2),\n",
       " (13, 16),\n",
       " (16, 13),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (19, 0),\n",
       " (19, 13),\n",
       " (19, 2),\n",
       " (0, 18),\n",
       " (0, 10),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 10),\n",
       " (2, 5),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (5, 18),\n",
       " (5, 10),\n",
       " (5, 2),\n",
       " (5, 16),\n",
       " (10, 0),\n",
       " (10, 18),\n",
       " (10, 5),\n",
       " (10, 2),\n",
       " (16, 5),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (18, 0),\n",
       " (18, 10),\n",
       " (18, 5),\n",
       " (0, 18),\n",
       " (0, 14),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 18),\n",
       " (2, 14),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (14, 0),\n",
       " (14, 18),\n",
       " (14, 2),\n",
       " (14, 16),\n",
       " (16, 14),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (18, 0),\n",
       " (18, 14),\n",
       " (18, 2),\n",
       " (0, 19),\n",
       " (0, 15),\n",
       " (2, 15),\n",
       " (2, 5),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (5, 19),\n",
       " (5, 15),\n",
       " (5, 2),\n",
       " (5, 16),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (15, 0),\n",
       " (15, 19),\n",
       " (15, 5),\n",
       " (15, 2),\n",
       " (16, 5),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (19, 0),\n",
       " (19, 15),\n",
       " (19, 5),\n",
       " (0, 18),\n",
       " (0, 10),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 10),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (10, 0),\n",
       " (10, 18),\n",
       " (10, 17),\n",
       " (10, 2),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (17, 18),\n",
       " (17, 10),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (18, 0),\n",
       " (18, 10),\n",
       " (18, 17),\n",
       " (0, 18),\n",
       " (0, 10),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 9),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (9, 18),\n",
       " (9, 10),\n",
       " (9, 17),\n",
       " (9, 2),\n",
       " (10, 0),\n",
       " (10, 18),\n",
       " (10, 9),\n",
       " (10, 17),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (17, 10),\n",
       " (17, 9),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (18, 0),\n",
       " (18, 10),\n",
       " (18, 9),\n",
       " (0, 18),\n",
       " (0, 14),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 14),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (14, 0),\n",
       " (14, 18),\n",
       " (14, 17),\n",
       " (14, 2),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (17, 18),\n",
       " (17, 14),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (18, 0),\n",
       " (18, 14),\n",
       " (18, 17),\n",
       " (0, 19),\n",
       " (0, 15),\n",
       " (2, 5),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (5, 19),\n",
       " (5, 15),\n",
       " (5, 17),\n",
       " (5, 2),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (15, 0),\n",
       " (15, 19),\n",
       " (15, 5),\n",
       " (15, 17),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (17, 15),\n",
       " (17, 5),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (19, 0),\n",
       " (19, 15),\n",
       " (19, 5),\n",
       " (0, 12),\n",
       " (0, 2),\n",
       " (2, 0),\n",
       " (2, 12),\n",
       " (2, 0),\n",
       " (2, 21),\n",
       " (12, 0),\n",
       " (12, 2),\n",
       " (12, 0),\n",
       " (21, 2),\n",
       " (21, 0),\n",
       " (0, 12),\n",
       " (0, 4),\n",
       " (4, 0),\n",
       " (4, 12),\n",
       " (4, 5),\n",
       " (4, 6),\n",
       " (5, 12),\n",
       " (5, 4),\n",
       " (5, 6),\n",
       " (5, 0),\n",
       " (6, 4),\n",
       " (6, 5),\n",
       " (6, 0),\n",
       " (6, 21),\n",
       " (12, 0),\n",
       " (12, 4),\n",
       " (12, 5),\n",
       " (21, 6),\n",
       " (21, 0),\n",
       " (0, 12),\n",
       " (0, 22),\n",
       " (12, 0),\n",
       " (12, 22),\n",
       " (12, 0),\n",
       " (21, 22),\n",
       " (21, 0),\n",
       " (22, 0),\n",
       " (22, 12),\n",
       " (22, 0),\n",
       " (22, 21),\n",
       " (0, 1),\n",
       " (0, 8),\n",
       " (1, 0),\n",
       " (1, 8),\n",
       " (1, 9),\n",
       " (6, 8),\n",
       " (6, 9),\n",
       " (6, 0),\n",
       " (6, 23),\n",
       " (8, 0),\n",
       " (8, 1),\n",
       " (8, 9),\n",
       " (8, 6),\n",
       " (9, 1),\n",
       " (9, 8),\n",
       " (9, 6),\n",
       " (9, 0),\n",
       " (23, 6),\n",
       " (23, 0),\n",
       " (0, 1),\n",
       " (0, 10),\n",
       " (1, 0),\n",
       " (1, 10),\n",
       " (1, 11),\n",
       " (10, 0),\n",
       " (10, 1),\n",
       " (10, 11),\n",
       " (10, 0),\n",
       " (11, 1),\n",
       " (11, 10),\n",
       " (11, 0),\n",
       " (11, 23),\n",
       " (23, 11),\n",
       " (23, 0),\n",
       " (0, 12),\n",
       " (0, 13),\n",
       " (5, 12),\n",
       " (5, 13),\n",
       " (5, 11),\n",
       " (5, 0),\n",
       " (11, 13),\n",
       " (11, 5),\n",
       " (11, 0),\n",
       " (11, 23),\n",
       " (12, 0),\n",
       " (12, 13),\n",
       " (12, 5),\n",
       " (13, 0),\n",
       " (13, 12),\n",
       " (13, 5),\n",
       " (13, 11),\n",
       " (23, 11),\n",
       " (23, 0),\n",
       " (0, 1),\n",
       " (0, 14),\n",
       " (1, 0),\n",
       " (1, 14),\n",
       " (1, 11),\n",
       " (11, 1),\n",
       " (11, 14),\n",
       " (11, 0),\n",
       " (11, 21),\n",
       " (14, 0),\n",
       " (14, 1),\n",
       " (14, 11),\n",
       " (14, 0),\n",
       " (21, 11),\n",
       " (21, 0),\n",
       " (0, 1),\n",
       " (0, 14),\n",
       " (1, 0),\n",
       " (1, 14),\n",
       " (1, 9),\n",
       " (9, 1),\n",
       " (9, 14),\n",
       " (9, 11),\n",
       " (9, 0),\n",
       " (11, 14),\n",
       " (11, 9),\n",
       " (11, 0),\n",
       " (11, 21),\n",
       " (14, 0),\n",
       " (14, 1),\n",
       " (14, 9),\n",
       " (14, 11),\n",
       " (21, 11),\n",
       " (21, 0),\n",
       " (0, 21),\n",
       " (0, 13),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 21),\n",
       " (2, 13),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (13, 0),\n",
       " (13, 21),\n",
       " (13, 2),\n",
       " (13, 16),\n",
       " (16, 13),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (21, 0),\n",
       " (21, 13),\n",
       " (21, 2),\n",
       " (0, 23),\n",
       " (0, 10),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 10),\n",
       " (2, 5),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (5, 23),\n",
       " (5, 10),\n",
       " (5, 2),\n",
       " (5, 16),\n",
       " (10, 0),\n",
       " (10, 23),\n",
       " (10, 5),\n",
       " (10, 2),\n",
       " (16, 5),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (23, 0),\n",
       " (23, 10),\n",
       " (23, 5),\n",
       " (0, 23),\n",
       " (0, 14),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 23),\n",
       " (2, 14),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (14, 0),\n",
       " (14, 23),\n",
       " (14, 2),\n",
       " (14, 16),\n",
       " (16, 14),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (23, 0),\n",
       " (23, 14),\n",
       " (23, 2),\n",
       " (0, 21),\n",
       " (0, 15),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 15),\n",
       " (2, 9),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (9, 21),\n",
       " (9, 15),\n",
       " (9, 2),\n",
       " (9, 16),\n",
       " (15, 0),\n",
       " (15, 21),\n",
       " (15, 9),\n",
       " (15, 2),\n",
       " (16, 9),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (21, 0),\n",
       " (21, 15),\n",
       " (21, 9),\n",
       " (0, 23),\n",
       " (0, 10),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 10),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (10, 0),\n",
       " (10, 23),\n",
       " (10, 17),\n",
       " (10, 2),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (17, 23),\n",
       " (17, 10),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (23, 0),\n",
       " (23, 10),\n",
       " (23, 17),\n",
       " (0, 23),\n",
       " (0, 10),\n",
       " (2, 9),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (9, 23),\n",
       " (9, 10),\n",
       " (9, 17),\n",
       " (9, 2),\n",
       " (10, 0),\n",
       " (10, 23),\n",
       " (10, 9),\n",
       " (10, 17),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (17, 10),\n",
       " (17, 9),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (23, 0),\n",
       " (23, 10),\n",
       " (23, 9),\n",
       " (0, 21),\n",
       " (0, 15),\n",
       " (2, 15),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (12, 16),\n",
       " (12, 0),\n",
       " (15, 0),\n",
       " (15, 21),\n",
       " (15, 17),\n",
       " (15, 2),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 12),\n",
       " (17, 21),\n",
       " (17, 15),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (21, 0),\n",
       " (21, 15),\n",
       " (21, 17),\n",
       " (0, 21),\n",
       " (0, 15),\n",
       " (1, 16),\n",
       " (1, 0),\n",
       " (2, 5),\n",
       " (2, 17),\n",
       " (2, 16),\n",
       " (2, 0),\n",
       " (5, 21),\n",
       " (5, 15),\n",
       " (5, 17),\n",
       " (5, 2),\n",
       " (15, 0),\n",
       " (15, 21),\n",
       " (15, 5),\n",
       " (15, 17),\n",
       " (16, 17),\n",
       " (16, 2),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (17, 15),\n",
       " (17, 5),\n",
       " (17, 2),\n",
       " (17, 16),\n",
       " (21, 0),\n",
       " (21, 15),\n",
       " (21, 5),\n",
       " (0, 12),\n",
       " (0, 2),\n",
       " (2, 0),\n",
       " (2, 12),\n",
       " (2, 0),\n",
       " (2, 24),\n",
       " (12, 0),\n",
       " (12, 2),\n",
       " (12, 0),\n",
       " (24, 2),\n",
       " (24, 0),\n",
       " (0, 12),\n",
       " (0, 4),\n",
       " (4, 0),\n",
       " (4, 12),\n",
       " (4, 9),\n",
       " (4, 6),\n",
       " (6, 4),\n",
       " (6, 9),\n",
       " (6, 0),\n",
       " (6, 24),\n",
       " (9, 12),\n",
       " (9, 4),\n",
       " (9, 6),\n",
       " (9, 0),\n",
       " (12, 0),\n",
       " (12, 4),\n",
       " (12, 9),\n",
       " (24, 6),\n",
       " (24, 0),\n",
       " (0, 12),\n",
       " (0, 22),\n",
       " (12, 0),\n",
       " (12, 22),\n",
       " (12, 0),\n",
       " (22, 0),\n",
       " (22, 12),\n",
       " (22, 0),\n",
       " (22, 25),\n",
       " (25, 22),\n",
       " (25, 0),\n",
       " (0, 12),\n",
       " (0, 20),\n",
       " (6, 20),\n",
       " (6, 9),\n",
       " (6, 0),\n",
       " (6, 25),\n",
       " (9, 12),\n",
       " (9, 20),\n",
       " (9, 6),\n",
       " (9, 0),\n",
       " (12, 0),\n",
       " (12, 20),\n",
       " (12, 9),\n",
       " (20, 0),\n",
       " (20, 12),\n",
       " (20, 9),\n",
       " (20, 6),\n",
       " (25, 6),\n",
       " (25, 0),\n",
       " (0, 12),\n",
       " (0, 13),\n",
       " (11, 12),\n",
       " (11, 13),\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# please see answer in q4\n",
    "# the rearrange_list is the input data\n",
    "rearrange_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2). Training your model. (5 pt)\n",
    "You could now train your model batch by batch using whatever optimizer you want.  \n",
    "In order to keep track of your training, you should also print out the loss every 1000*`X` batch.  \n",
    "(For a vocabulary with 1,000 words, it will take ~35mins for one epoch depending on your computer.)\n",
    "\n",
    "Write your code in the cell below. Print out the loss every 1000*`X` batch and your final average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# please see the result from question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing and Analysis (8 pt)\n",
    "### (1) Word embedding extraction\n",
    "Extract your word embedding matrix from the model and print out its shape.  \n",
    "(The size should be `[vocabulary_size, embedding_dimension]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 300])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1.weight.size() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Visualization\n",
    "In this step, you need to visualize your word vectors by dimension reduction. (e.g. PCA or t-SNE)  \n",
    "\n",
    "You can write your code using `matplotlib` and `scikit-learn`. Or you can also upload your word vectors and vocabulary to http://projector.tensorflow.org.  \n",
    "\n",
    "Write your code or attach your visualization images in the cell below and show the data points for `[\"man\", \"men\", \"woman\", \"women\"]` and `[\"i\", \"he\", \"she\", \"my\", \"his\", \"her\"]`.\n",
    "\n",
    "If you are not satisfied with the quality of your word vector from visualization (in most cases), you could try to change some parameters in your model (e.g. vocabulary_size, embedding_dimension) and re-train your word embedding.\n",
    "\n",
    "Tips:\n",
    "\n",
    "* You can use the code below to generate the vector file and word file needed in http://projector.tensorflow.org.\n",
    "```python\n",
    "np.savetxt(\"vectors.tsv\", <\"Your_word_embedding_in_numpy_format\">, delimiter='\\t')\n",
    "with open(\"words.tsv\", encoding=\"utf-8\", mode=\"w\") as words_f:\n",
    "    for word in itos:\n",
    "        words_f.write(word + \"\\n\")\n",
    "```\n",
    "\n",
    "* You might found those regular expressions helpful when searching words in http://projector.tensorflow.org.  \n",
    "`^man$|^woman$|^men$|^women$`  \n",
    "`^i$|^he$|^she$|^my$|^his$|^her$`\n",
    "\n",
    "\n",
    "* For attaching image into notebook file, move your image file into the same directory and write html code below in markdown mode (same mode for this cell).    \n",
    "`<img src=\"<your_screenshop_file>\" alt=\"\" style=\"width: 500px;\"/>`\n",
    "\n",
    "Examples:\n",
    "\n",
    "<img src=\"he_she_his_her.tiff\" alt=\"\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"man_men_woman_women.tiff\" alt=\"\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = model.linear1.weight.data.numpy()\n",
    "np.nan_to_num(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TSNE reported error as described as here\n",
    "https://github.com/scikit-learn/scikit-learn/issues/6665\n",
    "Need reboot python, so I dump and reload to avoid train the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix.dump('embedding_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('embedding_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embedding_matrix)\n",
    "X_embedded_df = pd.DataFrame(X_embedded)\n",
    "X_embedded_df.columns = ['x', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x111a7e780>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCgh/j9enZdAjrgmYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11441dbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.scatter(X_embedded_df['x'], X_embedded_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>30.885471</td>\n",
       "      <td>-83.979912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             x          y\n",
       "543  30.885471 -83.979912"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(itos[543]) # the wired point is 'm'. I dont know why there is a word called 'm'.....\n",
    "X_embedded_df[X_embedded_df.y< -30]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First plot for [\"man\", \"men\", \"woman\", \"women\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147e9f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_list1 = [\"man\", \"men\", \"woman\", \"women\"]\n",
    "target_num_list1 = []\n",
    "for item in target_list1:\n",
    "    target_num_list1.append(stoi[item])\n",
    "target_table_1 = X_embedded_df.loc[target_num_list1,:]\n",
    "\n",
    "x = list(target_table_1['x'])\n",
    "y = list(target_table_1['y'])\n",
    "\n",
    "fig, ax = plot.subplots()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "for i, txt in enumerate(target_list1):\n",
    "    ax.annotate(txt, (x[i],y[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'men' 'man' pair looks good, but 'woman' and 'women' pair is terrible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second plot for [\"i\", \"he\", \"she\", \"my\", \"his\", \"her\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqSQMfEkqCQNfkkrCwJekkvj/SBxen6lUfAEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1149aa4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_list2 = [\"i\", \"he\", \"she\", \"my\", \"his\", \"her\"]\n",
    "target_num_list2 = []\n",
    "for item in target_list2:\n",
    "    target_num_list2.append(stoi[item])\n",
    "target_table_2 = X_embedded_df.loc[target_num_list2,:]\n",
    "\n",
    "x = list(target_table_2['x'])\n",
    "y = list(target_table_2['y'])\n",
    "\n",
    "fig, ax = plot.subplots()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "for i, txt in enumerate(target_list2):\n",
    "    ax.annotate(txt, (x[i],y[i]))\n",
    "# The result is not too bad. i, he and she always locate at the left side of my his and her.\n",
    "# also, i he and she are always located in the higher place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vector evaluation (5 pt)\n",
    "Upload your vector file to http://www.wordvectors.org and evaluate your vector. You should make sure your file has the correct format. Report your result by attaching a screenshot of your result in the cell below.\n",
    "\n",
    "Again, if you are not satisfied with the quality of your word vectors from the evaluation (in most cases), you could try to change some parameters in your model (e.g. vocabulary_size, embedding_dimension, different optimizer, different total_number_epoch, batch_size) and re-train your word embedding.\n",
    "\n",
    "Example:\n",
    "\n",
    "<img src=\"vec_eval.png\" alt=\"\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>0.353322</td>\n",
       "      <td>-0.016044</td>\n",
       "      <td>0.390131</td>\n",
       "      <td>-0.205618</td>\n",
       "      <td>-0.123302</td>\n",
       "      <td>-0.557499</td>\n",
       "      <td>0.383200</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.247238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487824</td>\n",
       "      <td>0.111812</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.543891</td>\n",
       "      <td>0.069084</td>\n",
       "      <td>-0.081798</td>\n",
       "      <td>-0.247971</td>\n",
       "      <td>0.447530</td>\n",
       "      <td>0.217330</td>\n",
       "      <td>0.170554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>professors</td>\n",
       "      <td>0.046971</td>\n",
       "      <td>0.085804</td>\n",
       "      <td>-0.117736</td>\n",
       "      <td>-0.105729</td>\n",
       "      <td>0.148620</td>\n",
       "      <td>0.211598</td>\n",
       "      <td>-0.128798</td>\n",
       "      <td>0.163261</td>\n",
       "      <td>-0.101464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009347</td>\n",
       "      <td>-0.062453</td>\n",
       "      <td>-0.090737</td>\n",
       "      <td>-0.093756</td>\n",
       "      <td>-0.136789</td>\n",
       "      <td>-0.023517</td>\n",
       "      <td>0.094713</td>\n",
       "      <td>-0.110596</td>\n",
       "      <td>-0.137222</td>\n",
       "      <td>0.072067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>helped</td>\n",
       "      <td>0.040469</td>\n",
       "      <td>0.122977</td>\n",
       "      <td>-0.076497</td>\n",
       "      <td>-0.072647</td>\n",
       "      <td>0.208252</td>\n",
       "      <td>0.172277</td>\n",
       "      <td>-0.148894</td>\n",
       "      <td>0.194203</td>\n",
       "      <td>-0.174118</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109705</td>\n",
       "      <td>-0.039428</td>\n",
       "      <td>-0.134198</td>\n",
       "      <td>-0.145780</td>\n",
       "      <td>-0.071239</td>\n",
       "      <td>-0.072165</td>\n",
       "      <td>0.187307</td>\n",
       "      <td>-0.117002</td>\n",
       "      <td>-0.024968</td>\n",
       "      <td>-0.015101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>students</td>\n",
       "      <td>-0.066663</td>\n",
       "      <td>0.086328</td>\n",
       "      <td>-0.054912</td>\n",
       "      <td>-0.028442</td>\n",
       "      <td>0.272232</td>\n",
       "      <td>0.164861</td>\n",
       "      <td>-0.294504</td>\n",
       "      <td>0.217479</td>\n",
       "      <td>-0.146542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019489</td>\n",
       "      <td>-0.115755</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>-0.184633</td>\n",
       "      <td>-0.084602</td>\n",
       "      <td>0.092771</td>\n",
       "      <td>0.128362</td>\n",
       "      <td>-0.172608</td>\n",
       "      <td>-0.028499</td>\n",
       "      <td>0.059876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did</td>\n",
       "      <td>-0.273011</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>-0.100606</td>\n",
       "      <td>0.025949</td>\n",
       "      <td>0.353572</td>\n",
       "      <td>0.186209</td>\n",
       "      <td>-0.132086</td>\n",
       "      <td>0.097246</td>\n",
       "      <td>-0.068299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135128</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.022663</td>\n",
       "      <td>0.050879</td>\n",
       "      <td>-0.079405</td>\n",
       "      <td>0.080631</td>\n",
       "      <td>0.419414</td>\n",
       "      <td>-0.239794</td>\n",
       "      <td>-0.175060</td>\n",
       "      <td>0.154399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word         0         1         2         3         4         5  \\\n",
       "0         the  0.353322 -0.016044  0.390131 -0.205618 -0.123302 -0.557499   \n",
       "1  professors  0.046971  0.085804 -0.117736 -0.105729  0.148620  0.211598   \n",
       "2      helped  0.040469  0.122977 -0.076497 -0.072647  0.208252  0.172277   \n",
       "3    students -0.066663  0.086328 -0.054912 -0.028442  0.272232  0.164861   \n",
       "4         did -0.273011  0.023487 -0.100606  0.025949  0.353572  0.186209   \n",
       "\n",
       "          6         7         8    ...          290       291       292  \\\n",
       "0  0.383200  0.000805  0.247238    ...     0.487824  0.111812  0.119629   \n",
       "1 -0.128798  0.163261 -0.101464    ...    -0.009347 -0.062453 -0.090737   \n",
       "2 -0.148894  0.194203 -0.174118    ...    -0.109705 -0.039428 -0.134198   \n",
       "3 -0.294504  0.217479 -0.146542    ...    -0.019489 -0.115755 -0.002210   \n",
       "4 -0.132086  0.097246 -0.068299    ...    -0.135128  0.023918  0.022663   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.543891  0.069084 -0.081798 -0.247971  0.447530  0.217330  0.170554  \n",
       "1 -0.093756 -0.136789 -0.023517  0.094713 -0.110596 -0.137222  0.072067  \n",
       "2 -0.145780 -0.071239 -0.072165  0.187307 -0.117002 -0.024968 -0.015101  \n",
       "3 -0.184633 -0.084602  0.092771  0.128362 -0.172608 -0.028499  0.059876  \n",
       "4  0.050879 -0.079405  0.080631  0.419414 -0.239794 -0.175060  0.154399  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_eval1 = pd.DataFrame(embedding_matrix)\n",
    "matrix_eval1['word'] = pd.Series(list(stoi.keys()), index=matrix_eval1.index)\n",
    "cols = matrix_eval1.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "matrix_eval1 = matrix_eval1[cols]\n",
    "matrix_eval1.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix_eval1.to_csv(r'data.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third one is negative, which means we learned something opposite to the third matrix...\n",
    "<img src=\"eval_1.png\" alt=\"\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e+05 *\n",
      "  3.3000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      "  3.2976\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      "  3.2971\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      "  3.2967\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 200])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try bigger vocab size smaller batch size, smaller embedding size\n",
    "top_words_list = list(dict(total_counter).keys())[0:2000]\n",
    "num_list = range(0,2000)\n",
    "stoi = dict(zip(top_words_list,num_list))\n",
    "itos = dict(zip(num_list,top_words_list))\n",
    "top_words = list(stoi.keys())\n",
    "k = 5\n",
    "if k % 2 == 1 and k > 0:\n",
    "    half_k = (k-1)//2\n",
    "else:\n",
    "    print ('k need to be a positive odd number')\n",
    "rearrange_list = list()\n",
    "for line in tokenization:\n",
    "    for item in top_words:\n",
    "        if item in line:\n",
    "            line_len = len(line)\n",
    "            loc = line.index(item)\n",
    "            near_range = list(range(loc - half_k, loc+ half_k+1))\n",
    "            for pos in near_range:\n",
    "                if pos >= 0 and pos <= line_len -1 and pos != loc:\n",
    "                    if line[pos] in top_words:\n",
    "                        rearrange_list.append(tuple([stoi[item], stoi[line[pos]]]))\n",
    "input_list = [item[0] for item in rearrange_list]\n",
    "context_list = [item[1] for item in rearrange_list]\n",
    "CONTEXT_SIZE = 64 #(batch size)\n",
    "EMBEDDING_DIM = 200\n",
    "vocab_size = 2000\n",
    "vocab = top_words_list\n",
    "word_to_ix = stoi\n",
    "torch.manual_seed(1)\n",
    "class SkipgramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(SkipgramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = self.linear1(embeds)\n",
    "        probs = F.softmax(out)\n",
    "        return probs\n",
    "losses = []\n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "model = SkipgramModel(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(1,5):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for t in range((len(input_list)//CONTEXT_SIZE)):\n",
    "        sublist = rearrange_list[t*CONTEXT_SIZE: (t+1)*CONTEXT_SIZE]\n",
    "        context = list(map(operator.itemgetter(0),sublist))\n",
    "        target =  list(map(operator.itemgetter(1),sublist))\n",
    "        context_var = Variable(torch.LongTensor(context))\n",
    "        model.zero_grad()\n",
    "        probs = model(context_var)\n",
    "        loss = loss_function(probs, Variable(torch.LongTensor(target)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "    print(total_loss)    \n",
    "model.linear1.weight.size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I stoped even loss is still decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x114aaeda0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X9wXNWVJ/Dv6XYbt8wE2YlDoMHYOEQpvI5RogET126F\nMEEZHEDFb2JPkZ1U2EzNTIUfpcQGNsAWM9aMBkKqdnaryI8ttuJJbMARZEzWIUC2agiQyMjGccAL\nBhtom6DBiCSowW3p7B/dT379+t33q1+r+/X7fqoorO5W93tS6/R95557rqgqiIio82VafQBERDQ7\nGPCJiFKCAZ+IKCUY8ImIUoIBn4goJRjwiYhSggGfiCglGPCJiFKCAZ+IKCXmtPoA7D70oQ/pkiVL\nWn0YRESJsmPHjn9X1UV+j2urgL9kyRKMjo62+jCIiBJFRA4EeRxTOkREKcGAT0SUEgz4REQpwYBP\nRJQSDPhERCnRVlU6RGGNjBUxvH0vDk6UcHJ3HoP9PRjoLbT6sIjaEgM+JdbIWBEbtu5GqTwFAChO\nlLBh624AYNAncsGUDiXW8Pa9M8HeUipPYXj73hYdEVF7Y8CnxDo4UQp1O1HaMeBTYp3cnQ91O1Ha\nMeBTYg329yCfy9bcls9lMdjf06IjImpvnLSlxLImZlmlQxQMAz4l2kBvgQGeKCAGfHLF+naizsOA\nT3VY307UmRjwqY5XfTsDPtnxSjBZGPCpDuvbKQheCSYPyzKpDuvbKQiudE4eBnyqw/p2CoJXgsnD\ngE91BnoL2HjpChS68xAAhe48Nl66gpfpVINXgsnDHD65Yn07+Rns76nJ4QO8Emx3sYzwReQGEdkj\nIr8RkR+KyDwRWSgij4rIi9X/L4jjtYioPfBKMHlEVRt7ApECgH8DcKaqlkRkC4BHAJwJ4LCqDonI\negALVPUbXs/V19eno6OjDR0PEVHaiMgOVe3ze1xcOfw5APIiMgdAF4CDAC4BcF/1/vsADMT0WkRE\nFEHDAV9ViwD+CcCrAA4BeEdVfwbgRFU9VH3YGwBObPS1iIgouoYDfjU3fwmApQBOBjBfRNbZH6OV\nvJFr7khErhORUREZHR8fb/RwiIjIII6Uzp8BeEVVx1W1DGArgE8D+J2InAQA1f+/6fbNqnqvqvap\nat+iRYtiOBwiInITR1nmqwBWiUgXgBKA8wGMAngXwLUAhqr/fyiG1+po7EtCRM3UcMBX1WdE5AEA\nzwI4CmAMwL0AjgewRUS+DOAAgCsbfa1Oxr4kRNRssSy8UtXbANzmuPl9VEb7FAA7VBJRs7G1Qptg\nXxIiarZUtVZo5xz5yd15FF2CO/uSEFFcUjPCt3LkxYkSFMdy5CNjxVYfGgB2qCSi5ktNwG/33t3s\nS0JEzZaalE4ScuTsUElEzZSaET57dxNR2qUm4DNHTiNjRaweehxL12/D6qHH22b+hmi2pCalY6VK\n2rVKh5qLC9uIUhTwAebI04wL24hSlNKhdEvCpD1RszHgUypw0p6IAZ9SgpP2RCnL4VN6cdKeiAGf\nUoST9pR2TOkQEaUEAz4RUUow4BMRpQQDPhFRSjDgExGlBAM+EVFKMOATEaUEAz4RUUow4BMRpQQD\nPhFRSsQS8EWkW0QeEJEXROR5ETlXRBaKyKMi8mL1/wvieC0iIoomrhH+twH8H1X9OICVAJ4HsB7A\nY6p6BoDHql8TEVGLNBzwReQEAP8JwPcAQFWPqOoEgEsA3Fd92H0ABhp9LSIiii6OEf5SAOMA/peI\njInId0VkPoATVfVQ9TFvADgxhtciIqKI4gj4cwB8EsD/VNVeAO/Ckb5RVQWgbt8sIteJyKiIjI6P\nj8dwOERE5CaOgP86gNdV9Znq1w+g8gHwOxE5CQCq/3/T7ZtV9V5V7VPVvkWLFsVwOERE5KbhgK+q\nbwB4TUSsveLOB/BbAA8DuLZ627UAHmr0tYiIKLq4drz6WwCbRGQugJcB/GdUPky2iMiXARwAcGVM\nr0VERBHEEvBVdSeAPpe7zo/j+YmIqHFcaUtElBIM+EREKcGAT0SUEgz4REQpwYBPRJQSDPhERCnB\ngE9ElBIM+EREKcGAT0SUEnG1VqAOMzJWxPD2vTg4UcLJ3XkM9vdgoLfQ6sMiogYw4FOdkbEiNmzd\njVJ5CgBQnChhw9bdAMCgT5RgTOlQneHte2eCvaVUnsLw9r0tOiIiigMDPtU5OFEKdTsRJQMDPtU5\nuTsf6nYiSgYGfKoz2N+DfC5bc1s+l8Vgf4/hO4goCThpS3WsiVlW6RB1FgZ8cjXQW2CAJ+owTOkQ\nEaUEAz4RUUow4BMRpQQDPhFRSjDgExGlBAM+EVFKMOATEaVEbAFfRLIiMiYi/1r9eqGIPCoiL1b/\nvyCu1yIiovDiHOF/DcDztq/XA3hMVc8A8Fj1ayIiapFYAr6InAJgDYDv2m6+BMB91X/fB2Agjtci\nIqJo4hrh3wPg6wCmbbedqKqHqv9+A8CJMb0WERFF0HDAF5EvAHhTVXeYHqOqCkAN33+diIyKyOj4\n+Hijh0NERAZxjPBXA7hYRPYD+BGAz4rIDwD8TkROAoDq/990+2ZVvVdV+1S1b9GiRTEcDhERuWk4\n4KvqBlU9RVWXALgawOOqug7AwwCurT7sWgAPNfpaREQUXTPbIw8B2CIiXwZwAMCVTXwt6lAjY0X2\n5SeKSawBX1V/AeAX1X+/BeD8OJ+f0mVkrIgNW3fPbKhenChhw9bdAMCgT7G7dWQ3fvjMa5hSRVYE\n15xzKu4cWNHqw4oVN0ChtjW8fe9MsLeUylMY3r6XAT+h1n7nKTy57/DM16uXLcSmr5zbwiOquHVk\nN37w9KszX0+pznzdSUGfAZ/a1sGJUqjbKZyRsSK+8eBzeP/osWrqZgZgZ7AHgCf3Hcba7zzV8qD/\nw2deM97OgE80C07uzqPoEtxP7s634Gg6y8hYETdu2YlpR7F0MwOwM9j73d6oMCmaKXWtGjfenlRs\nnkZta7C/B/lctua2fC6Lwf6eFh1R5xjevrcu2FuaFYBnk5WisQK2laK5dWS36+OzIsbnWrJ+G5Zt\neMT4vUnCET61LStPzyqd+HV6WswvReMc/Z++qAsvvvmu8fk6JafPgE9tbaC3wADfBKZ0WTOtXrbQ\n9eph9bKFsb+WV4rGbYL2xTffxRkfno+Xxyc90zhJz+kzpUOUQoP9PcgYshjNCMAAsOkr59Y9d7Mm\niU0pmqyIcfT/8vgk9m28EOtWLTY+b9Jz+hzhE6WQddU0m1U6AGatGueac06tGcVb5uUyePfIlMt3\nuI/+nbxy/UnAgE+UUp2QLjNV4lhpF+s+AZDJiDHYW7yCPVD5IEky0Ta6ROnr69PR0dFWHwYRJYBp\nNL5u1eK6PPvqoccbnrM448PzMXlkui0LCERkh6r2+T2OI3xKDPbV6RxxtDEIs1gqjqokexVPUtt8\ncNKWEsHqq1OcKEFx7A9uZKzY6kOjkEw18mu/81So5/FbLDUyVsTqocexdP22xg7YoFSewvWbd2L1\n0OOJeR8ypUOJYLokL3Tn8eT6z7bgiNKhGb1vlm14xBis77nqLM8Rs/0qzxS5MgJ8YF4OE6VyQ8cZ\nRi4jyGUFk+XKBPiCrhxuu2j5rI3+g6Z0OMKnRGBfnXDso9uoI1Cv3jeN8CptvP3hPcb7nFd5RopZ\nDfYAUJ7WmWAPAG9PljH4wK62G/kz4FMimPrnsK9OvbjSX83qfeNV2ugVqN26pzqfN5/L1Gys3Url\nKcXw9r2tPowaDPiUCOyrE5xXW+l2ELW00etqrtCdx7QqSuV2CfcV7XYFyiodSgT21QmuVemvoJU3\ndw6swP2jr9cs+LIs6MoZn9/UDkKAWW8TEZTbFWgrN1phwKfE6ISFQrMhrrbSYXrfhN1A5B8u+wQG\nH9iF8tSxbHwuK7jtouXG4znv44uw6elXa/L3Anjn81sol5W6K9BWb7TCgE/UBF6juGaP8Ab7e2q2\nhgTM6S+vY9n0lXMDV+mE3UDE64rNbb0FADy4o5iYYG+q0mn1RisM+EQ2cQRjr1EcgKaP8IKmv4KM\nNoOWYEbZQMR5nMPb92L0wGE8uKNYt4/xvFymbl6iHYO9AHhlaI3x/lZvtMKAT1QV1+W21yjO63v8\nXmNkrIibtz43U/4nAqw9p76NABAs/RXnaDMr4hq0vCpynLtuFSdKrq0SSuUpz+qcdpIRwdL124wf\nslF+TnFiwKe2FmXhT9TFQnEFwCijOL8RntuWhKpo6OogztGmqTulW0WOlbJp14nWRlg/O1PrhTA/\np2ZgWWbK3DqyG8s2PJKIbduiLPxpZLFQXAHQqxe7131evLYk9Lpy8BL1WNzcObAC61YtnvnerIhr\nEzP7GoFO51YKG/Tn1Cwc4adIMyoEPnf3L2qaSp3x4fl49MbPNHScligLfxpZLBTX5bbfKC7KCM+r\npDJq/jfu0aa9LbE1inemN/wWT3Uat9+b/ec02xoO+CJyKoD/DeBEVOZR7lXVb4vIQgCbASwBsB/A\nlar6dqOvR9H9yzPuvb7/5ZlXI70BncEeqHQU/NzdvzAG/XbueBlXAHT2Yneb/A07Mey1JWHU/G+Q\n4zTxmtweGSvWlFwWJ0oYfGAXgPZbiNRs7bYSPI4R/lEAN6nqsyLyJwB2iMijAL4E4DFVHRKR9QDW\nA/hGDK9HEZlSAqbb/Zg2fTbdbl3OOyswgPZoMdtIAHR7LtP3RRnhDfb31OXwLX4fSF7BOcqx+F0p\n3vGTPTX19UClzcAdP9nTkr10W6UdV4I3nMNX1UOq+mz1338A8DyAAoBLANxXfdh9AAYafS1KtrBL\n/k17q3rtuRrle+zuHFiBfRsvxP6hNdi38cK22bB6oLeAu688C125Y3+yIu6bfdiZWhE3MnfjV4X0\n9qR7P5y3J8sY7O9BLpvsbQJNRCr194JKq4dPLj4BN23Z1VbzZbHm8EVkCYBeAM8AOFFVD1XvegOV\nlA+1UD6Xce01ks/Nztx90CX/9hGpk1/FTZjFQl6atTiqkeeNstK4GQt9vCa3gzRom5qq//5cVuqu\nCpJGFXivPI1vXXUWRg8cbumKWpPYAr6IHA/gQQDXq+rvxZZXVFUVEdffpohcB+A6AFi82LxbPDVu\n46WfwI2bd9Z0E8xUb4/ijA/Pd03fnPHh+a6PD7LkP8y2dSbO4G61Cg46b9Cs5e+tWFYfZ+mlNf/i\nxau9cXc+V6k2crmvPKXGSfMksa5Y33jnPdf7Z2tFrUksAV9EcqgE+02qurV68+9E5CRVPSQiJwF4\n0+17VfVeAPcClQ1Q4jgechd3A7JHb/xMqCqdIEv+/dIFYUfHI2NFDN6/C+Vp2wTi/ZUJRNN5N2v5\neyuW1YetPBoZK+KOn+yZSct053O4/eJKfxvn786NV3vj2y9ejhs27zTen/Rgb/Gao2j1OcZRpSMA\nvgfgeVW923bXwwCuBTBU/f9Djb5WJ4q7asUvZRB3AzKvEky3Y9l46QrP8/UakUYZHd/+8J6ZYG8p\nTytuf3iP8efQrOXvzVxWb3ofeVUeOb/nvI8vwuZfv1aTWpkolXHjlp1QbbyVwfD2vTghP7s7UbWb\n2VpRaxLHCH81gL8AsFtErI/vm1EJ9FtE5MsADgC4MobX6ihe5WtRgnKrO/EFPRbnloReOXs/9tGx\nfQWnX3rAK+iEHRUHzcsHfd6weX636qfrN+/EzVufw99X03XO5+s7bWHd9zg7UVqiVnE5paU6x8ts\nrag1aTjgq+q/odIzyM35jT5/p7GPqiCViR47q3wtSsCPkjJo1uRk0GMx5eyDsiYK7WkI6/aowtTj\nh/mQDfK8UT60TYuZJsvTGHxgF4YvX1n3vauHHk9EM7Kksz7kZ7vvvQlX2s4i50jM9BdmKmvzEzZl\nECW4BK2ACXosUdsC2AXJLTt1eVQmhanHD/MhG3RBVtDns3gtZrK22XMOINK2AKoVCt35uqvZVmPA\nnwVuI1A/t47sxhMvjIfK7YdNRYQNLl59apxBP+ixxJG/jrJU/zjHdolOQRckhf2Q9XveKHl+v8VM\nbsE9rgVQplLftGvHRVcAA37TOfP0QdlH3kFXpIZtDRA2uITpU2M6lilVLFm/DUBllJ2R+HLEYUxE\nvIpyijsvH6Wfz2B/D27YvNOYknFb3u9WMSUAPr1sIX576A+BBycM9vUKjgFaK7c0dGK3zCYb3r43\nlgUlQTahDtuJL85uiX7H4mayPN2SYA/E1+PE9GHqlpcPsto1yPM5DfQWsHaVeQ3L5JGjdQuiBnoL\nuOxTtYMHBfDUy4dj+zCk5qx0bgRH+E0WZ640yHP1nbZwJhX0kRPmoe80c0uBZvfmtqcvlm14xHjl\nkM9lcOSoRk7vuG1157X9XdjLba95i7jz8l7PNzJWxO0P76mpMLLq5O8cWIG+0xbW3Q9U5oTcrhC3\nPXcITq36AO4k9ivyVm9p6CTaRosd+vr6dHR0tNWHEavVQ4/HVo7mNwlUNylcNTcr+MfLV7qmg8Jc\nbrrl8IFjAdDruaw0jhsB8K2rzsJNW3aFDvr5XBaXfaqAbc8dqklD2HeDsldGdXfloAq8UyoHmhvx\nO+cgvM59v8d2eHbOBWR2uYxg+Ipjv1/Te25BVw5dc+fMzAuxTLK5Cj4/46C/+yBEZIeq9vk9jiP8\nJhvs78H1HqsLgwoyKjWV5x2ZUty4pXIMzuAWpluiV58av4ofr7r4fC7jmYM2Kdg2uHa2fnbuBmVt\njh22W2cj/fUtQfLyztG7cxPs4e17XYM9UFlIZq/EMV0Jvj1ZnvlQZLBvvoMe60FatQCLAb/JBnoL\noSt03Gy8dIVvlY5Xymda4VqeF5ZpVOt36WpKHwGY2aM1DPvoaPXQ4567QVkfaF7dOuNafez2geiX\nOnMbvb89WcZNthYQfuk8+/2tHr17pdPSxFq93MotDZ04aTsLbrtoOfI+ZYBeCt35QAHJbyLSK2hY\nDcaWrt+G1UOPB+p6aOdV8bNswyPGYD9/bvSfiyXoblBBu3VGZSpbfWX8j56T6abR+9R0ZREe4P+7\nte4fGSti8sjRhs+lEQz2x67IW72loRNH+LPACtZRRvphJhj90kduQWNkrIibtz5XM8o2pTq8cvRe\nKRuvvPy7R8w19KaRYnc+V/N10N2ggnTrdFq9bKExh+/klf7Z9JVzIy2cst4vg/09njn8wf4e4xxO\nPpfBe0en61Z1U3M4yzJbuaWhE0f4AXiNfoOOjAd6Cxj75gW456qzUOjOz2ySsG7VYhSqAcc+CkD1\n/iCpHPtrmDb6yAjqPjisNQJuKRVnGahfeVncl6gC4KMubZZzGZnp3mhZ8kFzwLYf12B/T92Vlt8H\n6qavnFv3M43SX99LkBLRgd4Chq9YWfdh153PzUzYmuZw3isz2M8WQaVXVDvs4OaGI3wbt46DAFwb\nU93xkz1Y84mTsPlXr4VqvWvqVml/7Y+cMC9S18xbR3bj6Zfrtw3uymXw95d+ou75/NYI2Eeefjl6\nt3LCRsosP2rotX/20gV1Vx2mkbXz0tn6vg1bn5tZMPT+0SmMHjjccAVTI7yuzKwAPzJWxC0/3j1z\nRSQA1jrOz3SlwFg/e6Ks74hjw56gGPCrTBUcx83JuI6a3p4su+alna13Tf3F7QEmTNdMUxvcKBuH\n+OWu7W/eIOka56WrV+29F6tE043zA830QZQVcT3v0QOHa1aHTjuqeSxR+gwFSf+YqnHWrVpc9/uz\nrmZGxoq46f5dmLKlcxSV435l/I8zwaHVk7VpF6WdQph2JXFgSqfKVMERpXe39T1WILfn7SdKZQze\nv6sm9WPa9PnGLTtrUkXW8xUnSlAc+2AYGSti0zPuk6Km2wHv0YigNgUUZVVulDSPNUEddD4g7LyB\n3wYrYR9n55f+sapx7O+ptyfLGHxgF/pOW1iX7rOnaqYMZUhP7js881467+OLjMdGzRU2/WqJo+w3\nDI7wq+LuHrh0/TZkDGkN6yrAGqmbxsDW37h1tZERuH4w3PGTPcYcrWrlWOxXA/a+8SZrVy2uefOu\nOn2B65tw1ekLjM8RNs2TrU4+AsF7yoStc47zg8T+cxRbq2u3qzjAXI1jdbQ05X793pvWXMvmXzfe\neZTCaWXFTRQM+FXdXTnXCpr5c7M4cnTauOjFROFdnTJRKoe6evDqCOlX+WNdDWzYuhv3j76KX+47\nbPyQMQWr/W+5Bx3n7W55730bL5y537QKVADcZVstGrTtQ9j2EHF9kDhTgPaHWldxQG1Kzitw2+9z\npu38dok6OFGKrWcTBSdo7YbkUXREwI9jcu19Q0A9cnQKnfJ3VCpPGS8V/do2BKlhD5L3Nu1ra10O\nm1oZmH6vfactxIM7Xp/Jy2cE+OI55lFXXB8kpooYi331q/X+9Hob2evonfM52Yx4LmbKiDB33wJx\nNOALU/Ybh8QH/Di29RsZKxpXe6al++vBiZLn/rpeNex+WxTaV7t6baQetm/NsVH2sV+S3xzxnQMr\n8MzLb9VtvB52gBAkBXhwohRoR69c9lgqy20+Z2paMX9uFuWpaRxxGX1MqXJ16yyLq9+9V7uSZkh8\n8zRTJUhWpCaVYGJarJI2C7pyeK88Xdcf3Sr9c/s55bKVxHWQD8UgjaLCNhnzakznVrYImLdUdOZi\n/d5XQZriFbrzeOOd9zxTe86eOX4/g5GxYqQmc9S4rAimVQNvSDSbgjZPS3yVTpTVncCxBVPXb96Z\n+mAvqIyM3fY4/cHTr6L3v/0MQKWfj1VFsqArB2iwK6BmNYryGmVbx+7sOx60+sbvfXXexxcZN3IG\njq1+9Xofrlu1GL8vHcX1m3di2YZHfHukrx56HAAw7fMBQvHL57K468qVeGVoTVsvrPKT+IAfpVzQ\nGq0y71kxJwPPScG3J49NQj65/rN4ZWgNuubOCTyRHccqXLfVzEFyqGEDucXrfTUyVsSDO4qeKZSr\nzj4VA70Fz/eh28rluVnz460y3G5DUM+K+G5ectycxP/Jz7qoJZftKPE5/CibePhNuKVNkFG6VUo6\neuCwZ77ezmsC3W2i3TSBlZFj7XyLEyXcsHknRg8c9t3aD6gvozRxhtnTF3W5rvS95pxTA71/tj13\nyLNLqCnnXp5Sz20fy1OKP75Xdv3+IL+T94+mZFIqJlarBMC86DFJEv9xH6UbXdw192kxUSrXjEq9\nrFu1GPs2XmgM9m6j26WLjq+rTjhuTqYu+CmATdUgunbVYs/Uir2M0lq97KbL1rXz1pHdrsHemtwN\n8v55e7KMkbEinnhhvOb2jFSamZl+ggr/XafK05ygnS326ikrK2Avcw7bVbbVEj/CB8J3o+MS9Oax\nl0WaymW98ujOifalhklMBWYWK/WdtrCmz4ydvYzSq0590va9puN7eXwSQPD3j3Pz+mxGkAE3/k4K\neyXObOylMBuaHvBF5PMAvg0gC+C7qjrU7Nf041YLTo1xtoT1KpcNM9HuFVytkbbVkO5zd//CWG4Z\npG+QdcnudXxhtqx0K6/kOy4ZnO/nZu+lMFuamtIRkSyAfwbw5wDOBHCNiJzZzNcMYqC3gMs+lZxP\n5XaR8cidODfdMPXw+eEzr3lOZDqrVQb7e4wpG/ukrVsa5sU338Xa7zxV91g3Sz6YDzSRzyvDziao\nlL86K3FM7584Fl/Npmbn8M8G8JKqvqyqRwD8CMAlTX7NQJy5VfK2oCuHL56z2Hi/1QTMavJmSvNP\nqXpOqDv77A/0Flz74jsXvpjSMFZzscH+nsq6AYNf7jvMK74UCTKIsIuyl0I7anbALwCw/yW+Xr2t\n5ZJ2KdZKgkpAf+KFcc8l31YTMPvGKW76TluIfM77rWcFcNME6icXn1AzAvOaSLaOZ/5ccwaTk6Cd\ny+2K0u337RXAB3oLNetQklqq2fJJWxG5DsB1ALB4sXkEGTdO3AZn/XEUJ0qxpDyc7RDcTKliZKxo\nHLk//fLbNWVyfsfEOZv08lqoZnGueHZj2rwoSZod8IsA7Nfvp1Rvm6Gq9wK4F6i0Vmjy8cwwNfEC\nlFUUTZQVCRx4Bx8wtxCYUg0VxBns0ykrgj+ZN8e3M62qeZc6N7O1G1rcmp3S+TWAM0RkqYjMBXA1\ngIeb/JqBjB44jPeP2oN9BhsvXYH3GOybKkwPGL92vwzi5GdKFe8eOYqcV8UBvFeaO/nt79zOmhrw\nVfUogL8BsB3A8wC2qOqeZr5mENYvzL7ApVSexuiBw4mbdad6Wam0E25WDx9KlvKU4vh5c1CI6W87\nym5o7aLpK21V9RFV/ZiqLlPVv2v26wXh9Qtzm42n9uPVJGxKFd1dOXaUpBkTk2XP/R7CiNqwsR0k\nvrVCFF6/MGs2ntqb39+W3y5glC7WlbtpoBCmy2iUho3tIpUB3+8XNtBbiO3yj5rjnVIZXT6lnURA\nbbnlbRctr1uPkcsKbrtoeeDnM60jiaMrbLOl8i8myC+MqZ32JgLjLmVEFme9/EBvAcOXr6yppx++\nfGWoCp0oDRvbReJ3vIoqSFnVyFgR12/eOSvHQ7UyUumUyRJZakTYQJzUFshBd7xKbcAPKkyzLIrX\nulWLffeDJcplxVjCG3SrU6AS7G+6fxempms7nN51RbgrgFZIzRaHzcbUTusw2FMQw5evNN4XpnLm\nlh/vrgn2QKXD6S0/bv/6+qAY8H3Ye2gA5qZLRNQc3XnvChqvrSTDVM647afgdXsSMeAHMNBbwJPr\nP4tCd55Ntohm2c7bLjAGfev2JFfOzCYG/BCidti0xhhWZUAzcZNq6jRL12+DSH2wymUEt19cKaeM\no3LG9LfZSVf1Le+WmSRRO2wqKsH+yfWfxdrvPOW6UXcQXptbW44cnfacxCJKGkVlIV0uK/jA3Dl4\np1TGCfkcRIAbNu/E8Pa9GOzvCb3VqdNaQ5HA2lWz18W32TgcDKGRCdyDEyWMjBXx7Kvv1N23etlC\nz4Vehe489g+twd1XnuX7+orKJJZ95WB3Pod7rjqrZgRElDTlKcX84+bgW1edhfePTuPtyXKsG4on\nub4+KI7wQ7BKs+x1uks+mMcv9x32ze2f3J133QgZAPa/VcJgf0/dptdA5bLVWiVof33TlUZWxLVv\n98hYEU/jVZoyAAAMZUlEQVS8MB6oNzhRuzo4UWrKhuLOdTmdFugtDPghmTZBGBkr4uatz7mu/rSW\ndt9gWMR1cKI085x3/GTPTB+Y7nwOt19cuymD9frOTcIt1iSVtYCkOFGCiH/vGZo9c7OCbCbD9s4R\nnNydj31DceffktXuGEDHBX0G/JjYPwhMq/VMI3OrsVOYHXWsN6LbauGRsWLN5iAM9u3lyJQCUwz2\nXgTAvFy2boOiwf4e37+jsLy65zLgky9T4DbtshV1I2TnJNXIWDHyymAB93Wl9qEANl66wtjmIM6/\noyS3Ow6LAX8Wuc0BePXqCNPXwzmqD8KqHLKwjQS1C2vi1K2Hfdi/oyCv5RbcO7HAgQF/lgVN2zgD\nuFWJYD2Hk2lC2ItzROR2BULUCtaexYD7+z3ODcWvOedUz/mwTsKyzDblVYngJuyEVXc+V/cHM9Bb\nwGWfKnTUQhNqf93Vmnonr/d7nNJQjmnhCL9Nha1ECLMozL5C0W5krIjNv3qNuXyaNfuH1gCorKZ1\nE7XyJqxGF20lBUf4bcpUcWC6PdyiMDWmhcp+S3mJYmLfsCzs+52iYcBvU24B3KsSwd7VU+DdU6c8\nXak9dpqt0RQRABw/79hq8LDvd4qGKZ02FaUSwTmRtWzDI8bSMrca46i9goiimLBtNB935Q25Y8Bv\nY41WInjVEbvdN9jfg8H7d9WldXJZwfy5czBRKtd9D5EfU9mjM10TZ+UNuWNKp4N5NWRzqzEe6C1g\n+IqVNb3HF3TlcPaSBfj9ewz2nShIrfmCrhxymdrHBa3kyueyuOacU5muaRMNBXwRGRaRF0TkORH5\nsYh02+7bICIvicheEelv/FAprMH+HuMv2F5jfOvIbizb8AiWrN+Gm7bswhdWnoT9Q2uwf2gN1nzi\nJDy577BvW2ZKnv1Da3z3exUAY9+8AMNXrJyZHwq6EVChO4+Nl1aqX+zzS9btHM3PvkZTOo8C2KCq\nR0XkHwBsAPANETkTwNUAlgM4GcDPReRjqsoVPbPo2DL051CqNnXLCPDFc47VGPs1jjL1GaFky4pg\nZKyIgd4CFnTlZhr2OZn6PPmtyhbUrpJluqY9NBTwVfVnti+fBnB59d+XAPiRqr4P4BUReQnA2QCe\nauT1KDy/PzS/xlFe8wD3XHXWzCRbxpCnpfY0pYobNu/E6IHDuO2i5bjp/l11G3jbW3M7+a3KZjll\ne4ozh/+XAH5a/XcBgD2SvF69jdqMX+Mor82hrb1+Xxlag7uuXBl5cxhqDQWwqXo1d9cVtZvmAEB5\nWnH95p0487/+tG5zEasM2Pk9APPz7cw34IvIz0XkNy7/XWJ7zC0AjgLYFPYAROQ6ERkVkdHx8fGw\n304N8groQPDNoe3rAKi9eH0QKzCzccjYNy/AOpft/CbL07hxy07XoD/2zQtwz1VnMT+fEKINXoaL\nyJcA/BcA56vqZPW2DQCgqhurX28HcLuqeqZ0+vr6dHR0tKHjoXBMG6nYe4k4dwM6fVEXXh6frOvD\nb4nSuZOay1QaCVTy7a9UWxx4rd1wdlel9iEiO1S1z+9xjVbpfB7A1wFcbAX7qocBXC0ix4nIUgBn\nAPhVI69FzRGkcdSdAyuwb+OF2D+0BteccypefPPdmaBgTfLaV+46V/0WuvNYvWyh6+vPn+s++uzE\n1rRxy2WCl0d6za/Y8+1ej+NK7ORraIRfnYw9DsBb1ZueVtWvVu+7BZW8/lEA16vqT92f5RiO8Nuf\naQSYFfEs8XNeSQiAtasWY9PTr7qW+Am48teP9TM3XaUFYW18U6iubL1pyy6O8BNoVkb4qvpRVT1V\nVc+q/vdV231/p6rLVLUnSLCnZIiyO5BbQLIe7dU0K1xDuPSZUsXS9dvwxAvjWL1sYeCrImuexb7L\nmbXfwqrTF7h+T0bq90+g5OFKWwrFb5LXjVfpp1fTLOdEMNM89RSVYP3sq+/gritX+j7eGqW7LZ4q\nlaew/60S1q1aXNOfviuXwd1XnsWJ2A7AXjoUSpTdgbyuCtyaZi35YB43bdmF6zfvrJsU5jaM7qzN\nQrrzOWPPI3u5pNd+C2npDZ9GHOFTKFF2B/K7KrDX85/38UV4ct9h46RwlDRPWq4LDk6UcPvFy+v6\n3gCVfjj2ckn2n0+nhssy48RJ284UpPTTEmRS+NaR3cbJ3qjyuQyOTivKU+3z9xCVCJCfk0GpPG1s\nM+xWOpvPZVlDn1BBJ22Z0qGms4K6vZbfWbtvCTIp/MQL48Zg71Vv7qY7n8P84+bg4EQJ3R49ZdqN\nfcLVSbWyWMrryov959OJI3xqK0FG+EvXbzOWcr4ytMZ4v1MuI4CgZlRvCqTWxHHc8wf5XBbzchnP\nD5pcRmr2KMjnsrjsUwU88cI4Dk6UPD/8/LphUmeYlbJMorgFaeXgl3823b+gK1ezGOz4eXPqUjiK\n+py/NdkZ98Ijqw3BbRctN84zFLrzda2JrZbD1ryHCZvZkRNTOtRWgqR/3Do12itQTPffdtHympTF\n0vXbXI/BWojkTHUMb98bywg/lxEMX7Gy5lhGDxyum5ewl6d6pVpMaSyWsZITAz61Hb+yQL/8c9D8\ntGklr2lF6WB/D27YvNOYQunKZTBZ3XfApDufw+0XVz54nD2KPr1sIfa/VQqdU49SKkvpxBw+pVaU\nShW3CiH794yMFWeuBKyRd8EleIepXArC+eFhmhSnzhQ0h8+AT6lmBegwo+oo3+MUtScRkRuWZRIF\nEGXrvTi264vSk4ioUazSIWqBKD2JiBrFgE/UAkF3EiOKE1M6RC0QZvUxUVw4aUtElHBcaUtERDUY\n8ImIUoIBn4goJRjwiYhSggGfiCgl2qpKR0TGARxo0tN/CMC/N+m5W43nlkydem6del5A+57baaq6\nyO9BbRXwm0lERoOULSURzy2ZOvXcOvW8gOSfG1M6REQpwYBPRJQSaQr497b6AJqI55ZMnXpunXpe\nQMLPLTU5fCKitEvTCJ+IKNVSEfBF5G9F5AUR2SMi/2i7fYOIvCQie0Wkv5XH2AgRuUlEVEQ+ZLst\nsecmIsPV39dzIvJjEem23ZfY87KIyOerx/+SiKxv9fE0QkROFZEnROS31b+vr1VvXygij4rIi9X/\nL2j1sUYhIlkRGRORf61+nejz6viALyLnAbgEwEpVXQ7gn6q3nwngagDLAXwewP8QkWzLDjQiETkV\nwAUAXrXdlvRzexTAf1DVTwD4fwA2AB1xXqge7z8D+HMAZwK4pnpeSXUUwE2qeiaAVQD+uno+6wE8\npqpnAHis+nUSfQ3A87avE31eHR/wAfwVgCFVfR8AVPXN6u2XAPiRqr6vqq8AeAnA2S06xkZ8C8DX\ngZp9tRN9bqr6M1U9Wv3yaQCnVP+d6POqOhvAS6r6sqoeAfAjVM4rkVT1kKo+W/33H1AJjgVUzum+\n6sPuAzDQmiOMTkROAbAGwHdtNyf6vNIQ8D8G4D+KyDMi8n9F5E+rtxcAvGZ73OvV2xJDRC4BUFTV\nXY67En9uNn8J4KfVf3fCeXXCObgSkSUAegE8A+BEVT1UvesNACe26LAacQ8qg6lp222JPq+O2PFK\nRH4O4CMud92CyjkuROVy808BbBGR02fx8Bric243o5LOSRyv81LVh6qPuQWVlMGm2Tw2Ck9Ejgfw\nIIDrVfX3YtubV1VVRBJVDigiXwDwpqruEJHPuD0miefVEQFfVf/MdJ+I/BWArVqpP/2ViEyj0g+j\nCMC+gegp1dvaiuncRGQFgKUAdlX/uE4B8KyInI0EnJvX7wwARORLAL4A4Hw9Vjvc9ucVQCecQw0R\nyaES7Dep6tbqzb8TkZNU9ZCInATgTfMztKXVAC4WkQsBzAPwARH5ARJ+XmlI6YwAOA8ARORjAOai\n0vzoYQBXi8hxIrIUwBkAftWyowxJVXer6odVdYmqLkElNfBJVX0DCT83Efk8KpfSF6vqpO2uRJ9X\n1a8BnCEiS0VkLiqT0A+3+Jgik8po43sAnlfVu213PQzg2uq/rwXw0GwfWyNUdYOqnlL927oawOOq\nug4JP6+OGOH7+D6A74vIbwAcAXBtdcS4R0S2APgtKmmDv1bVqRYeZ2xUNenn9t8BHAfg0erVy9Oq\n+tUOOC+o6lER+RsA2wFkAXxfVfe0+LAasRrAXwDYLSI7q7fdDGAIlfTpl1HpgHtli44vbok+L660\nJSJKiTSkdIiICAz4RESpwYBPRJQSDPhERCnBgE9ElBIM+EREKcGAT0SUEgz4REQp8f8BVHrm9PlB\n7bgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114b60940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_matrix = model.linear1.weight.data.numpy()\n",
    "np.nan_to_num(embedding_matrix)\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embedding_matrix)\n",
    "X_embedded_df = pd.DataFrame(X_embedded)\n",
    "X_embedded_df.columns = ['x', 'y']\n",
    "plot.scatter(X_embedded_df['x'], X_embedded_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>-47.095749</td>\n",
       "      <td>15.454429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>professors</td>\n",
       "      <td>-45.991272</td>\n",
       "      <td>13.956613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>helped</td>\n",
       "      <td>-46.086510</td>\n",
       "      <td>14.049241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>students</td>\n",
       "      <td>-46.152401</td>\n",
       "      <td>13.719707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did</td>\n",
       "      <td>-47.650784</td>\n",
       "      <td>14.271381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word          x          y\n",
       "0         the -47.095749  15.454429\n",
       "1  professors -45.991272  13.956613\n",
       "2      helped -46.086510  14.049241\n",
       "3    students -46.152401  13.719707\n",
       "4         did -47.650784  14.271381"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded_df['word'] = pd.Series(list(stoi.keys()), index=X_embedded_df.index)\n",
    "# change column name\n",
    "#cols = X_embedded_df.columns.tolist()\n",
    "#cols = cols[-1:] + cols[:-1]\n",
    "#X_embedded_df = X_embedded_df[cols]\n",
    "X_embedded_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>-0.235997</td>\n",
       "      <td>0.741338</td>\n",
       "      <td>-1.201902</td>\n",
       "      <td>0.214136</td>\n",
       "      <td>-0.335695</td>\n",
       "      <td>-1.228136</td>\n",
       "      <td>-0.801957</td>\n",
       "      <td>0.270888</td>\n",
       "      <td>-0.793187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>-0.692796</td>\n",
       "      <td>0.233849</td>\n",
       "      <td>0.628848</td>\n",
       "      <td>-0.303524</td>\n",
       "      <td>-0.260300</td>\n",
       "      <td>-0.189190</td>\n",
       "      <td>-0.404541</td>\n",
       "      <td>0.201796</td>\n",
       "      <td>-0.938598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>professors</td>\n",
       "      <td>0.045964</td>\n",
       "      <td>0.170190</td>\n",
       "      <td>0.126450</td>\n",
       "      <td>0.096925</td>\n",
       "      <td>-0.176407</td>\n",
       "      <td>-0.143540</td>\n",
       "      <td>-0.058546</td>\n",
       "      <td>-0.245748</td>\n",
       "      <td>-0.437762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135350</td>\n",
       "      <td>-0.081010</td>\n",
       "      <td>0.026984</td>\n",
       "      <td>-0.017704</td>\n",
       "      <td>-0.174584</td>\n",
       "      <td>-0.090991</td>\n",
       "      <td>-0.198293</td>\n",
       "      <td>0.212792</td>\n",
       "      <td>-0.085341</td>\n",
       "      <td>0.087955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>helped</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>0.166119</td>\n",
       "      <td>0.121265</td>\n",
       "      <td>0.107956</td>\n",
       "      <td>-0.155357</td>\n",
       "      <td>-0.166048</td>\n",
       "      <td>-0.057939</td>\n",
       "      <td>-0.245602</td>\n",
       "      <td>-0.437474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128260</td>\n",
       "      <td>-0.089041</td>\n",
       "      <td>0.035370</td>\n",
       "      <td>-0.006857</td>\n",
       "      <td>-0.166861</td>\n",
       "      <td>-0.086016</td>\n",
       "      <td>-0.198513</td>\n",
       "      <td>0.211310</td>\n",
       "      <td>-0.081120</td>\n",
       "      <td>0.082469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>students</td>\n",
       "      <td>0.055913</td>\n",
       "      <td>0.176406</td>\n",
       "      <td>0.147707</td>\n",
       "      <td>0.084541</td>\n",
       "      <td>-0.162693</td>\n",
       "      <td>-0.121906</td>\n",
       "      <td>-0.052722</td>\n",
       "      <td>-0.252282</td>\n",
       "      <td>-0.437255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128104</td>\n",
       "      <td>-0.073504</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>-0.029469</td>\n",
       "      <td>-0.169789</td>\n",
       "      <td>-0.074354</td>\n",
       "      <td>-0.195559</td>\n",
       "      <td>0.202324</td>\n",
       "      <td>-0.099354</td>\n",
       "      <td>0.107843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did</td>\n",
       "      <td>0.089890</td>\n",
       "      <td>0.132508</td>\n",
       "      <td>0.154712</td>\n",
       "      <td>0.181999</td>\n",
       "      <td>-0.239103</td>\n",
       "      <td>-0.185288</td>\n",
       "      <td>-0.086784</td>\n",
       "      <td>-0.055486</td>\n",
       "      <td>-0.389349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156768</td>\n",
       "      <td>-0.150295</td>\n",
       "      <td>-0.022600</td>\n",
       "      <td>-0.006629</td>\n",
       "      <td>-0.128488</td>\n",
       "      <td>0.033628</td>\n",
       "      <td>-0.198890</td>\n",
       "      <td>0.149780</td>\n",
       "      <td>-0.026736</td>\n",
       "      <td>0.008052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word         0         1         2         3         4         5  \\\n",
       "0         the -0.235997  0.741338 -1.201902  0.214136 -0.335695 -1.228136   \n",
       "1  professors  0.045964  0.170190  0.126450  0.096925 -0.176407 -0.143540   \n",
       "2      helped  0.042119  0.166119  0.121265  0.107956 -0.155357 -0.166048   \n",
       "3    students  0.055913  0.176406  0.147707  0.084541 -0.162693 -0.121906   \n",
       "4         did  0.089890  0.132508  0.154712  0.181999 -0.239103 -0.185288   \n",
       "\n",
       "          6         7         8    ...          190       191       192  \\\n",
       "0 -0.801957  0.270888 -0.793187    ...     0.006595 -0.692796  0.233849   \n",
       "1 -0.058546 -0.245748 -0.437762    ...    -0.135350 -0.081010  0.026984   \n",
       "2 -0.057939 -0.245602 -0.437474    ...    -0.128260 -0.089041  0.035370   \n",
       "3 -0.052722 -0.252282 -0.437255    ...    -0.128104 -0.073504  0.012002   \n",
       "4 -0.086784 -0.055486 -0.389349    ...    -0.156768 -0.150295 -0.022600   \n",
       "\n",
       "        193       194       195       196       197       198       199  \n",
       "0  0.628848 -0.303524 -0.260300 -0.189190 -0.404541  0.201796 -0.938598  \n",
       "1 -0.017704 -0.174584 -0.090991 -0.198293  0.212792 -0.085341  0.087955  \n",
       "2 -0.006857 -0.166861 -0.086016 -0.198513  0.211310 -0.081120  0.082469  \n",
       "3 -0.029469 -0.169789 -0.074354 -0.195559  0.202324 -0.099354  0.107843  \n",
       "4 -0.006629 -0.128488  0.033628 -0.198890  0.149780 -0.026736  0.008052  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = model.linear1.weight.data.numpy()\n",
    "matrix_eval2 = pd.DataFrame(embedding_matrix)\n",
    "matrix_eval2['word'] = pd.Series(list(stoi.keys()), index=matrix_eval2.index)\n",
    "cols = matrix_eval2.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "matrix_eval2 = matrix_eval1[cols]\n",
    "matrix_eval2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_eval2.to_csv(r'data2.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is based on matrix_eval2. What I can say is : This is life.....\n",
    "<img src=\"eval_3.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "### However, if I compress (dimension reduction) to two dimensions, I get this one\n",
    "<img src=\"eval_2.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "### It is pretty hard to say if I learned something or not in compared with the second matrix WoedVec Demo provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extra materials\n",
    "### (1). Reading  (3 pt)\n",
    "You have trained your own toy word embedding but there are two important tricks the homework didn't cover.  \n",
    "Read http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ and write a summary about:\n",
    "* Subsampling Frequent Words\n",
    "* Negative Sampling\n",
    "\n",
    "Explain why we need these two tricks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer\n",
    "In the first tutorial, we discussed how to train a word embedding matrix(look-up table). However, one of the most important disadvantages of this naive method is it considers all the words. In this homework, TA said it will take 35 min to train single epoch for only 1000 words embedding matrix. However, the real world case may have billion level corpus and million level words, which is a disaster. Therefore, in Google's second paper they introduced two methods: subsampling and negative sampling.\n",
    "\n",
    "The general idea of subsampling is to remove some frequent words such as 'and', 'the', and 'a'. These words will not introduce any further information to the sentence understanding, but create an enormous number of word pairs. By count the number of each word in the corpus, and use a function to calculate the probability of if to keep this word in the matrix, we keep all words with a fraction of total words lower than 0.0026. The other words with higher fraction will have a higher risk to be kicked out.\n",
    "\n",
    "The general idea of negative sampling is to take only a few negative words to update. Compared with the naive method, which updates one target word with 1 and all other words with 0. In this method, it only updates selected words(2-5 for large data set) to update for each time. This method also saves huge time in the calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see answer part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2). Subsampling and negative sampling (Optional with 10 pt bonus)\n",
    "This is an optional problem.  \n",
    "Reimplement your model using subsampling and negative sampling and then re-train your embedding with at least 5,000 words in your vocabulary. (`K=>5,000`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional analysis (5 pt)\n",
    "This course is designed to train you as an NLP researcher. A researcher should not only be able to implement newly emerged models and algorithms and get them to work but also give reasons and intuitions behind every decision you make during your research (e.g. parameter and structure design).  \n",
    "In this section, write down anything you think that is important in this homework.  \n",
    "It could be:\n",
    "* The problems you encountered during the implementation and how you resolve it.\n",
    "* You were not satisfied with the quality of your word vector from visualization and you made some changes to (or it fails to) improve it. Why do you think those changes can (might) be helpful?  \n",
    "\n",
    "Use your imagination and try to record every detail of your experiments. The bonus will be given to novel and reasonable thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. What is embedding layer. Embedding layer is not just 'one hot encoding' layer but we can treat as 'one hot \n",
    "# encoding' layer + 'linear' layer. Therefore, we only need two layers 'embeding' and 'linear1' for this task.\n",
    "\n",
    "# 2. Why we don't introduce nonlinear layer into embedding? Especially the second layer. It is because we want to keep \n",
    "# as much as possible information into the word embedding layer aka first layer. So it will work as a good look up table.\n",
    "# Because we want to get rid of the second layer, therefore, we want it has worest efficiency.\n",
    "\n",
    "# 3. Smaller batch size looks better. For smaller, I mean batch size ~ 100, originally I used batch size ~ 1000.\n",
    "\n",
    "# 4. Some advanced optimizer for example adam has better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
