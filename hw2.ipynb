{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 790.139: Natural Language Processing (Fall 2017): Coding Homework 2 (Sequence-to-Label Learning, Entailment Recognition)\n",
    "Created by TA Yixin Nie (Instructor: Mohit Bansal)\n",
    "\n",
    "\n",
    "# Instructions\n",
    "\n",
    "**All the instructions are present in the jupyter notebook (as shown in the class; and see the preview below).  \n",
    "Install jupyter notebook in your python environment and download the file below.**  \n",
    "https://drive.google.com/drive/folders/0B6i0pVGwapCdVHV1cnVNdWhvM1k?usp=sharing  \n",
    "  \n",
    "  \n",
    "**Use this directory as your workspace and write your code in the “hw2.ipynb” file. You could also add extra images or tables in the directory and link them into “hw2.ipynb” file but grading will only base on the “hw2.ipynb” file.**    \n",
    "\n",
    "**Make sure to name your directory as `\"<your_name>_hw2\"` and compress it to `\"<your_name>_hw2.zip\"`.   \n",
    "Email the file to <a href=\"mailto:comp790.hw@gmail.com\">comp790.hw@gmail.com</a> for submission.**    \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Preview\n",
    "The main goals for homework 2 are:\n",
    "1. Going through some setup procedures of training deep neural network for NLP.\n",
    "2. Getting familiar with Recurrent Neural Networks.\n",
    "3. Learning to handle variable length inputs in deep learning framework (Pytorch, Tensorflow).\n",
    "4. Building a sequence-to-label model for natural language inference task.\n",
    "5. Learning to improve your model by interpreting your experiment result.\n",
    "\n",
    "Notice:  \n",
    "You can test or run your code in any environment but you could only show your codes, your results and your write-ups in this single notebook file. We will not re-run your code for grading.  \n",
    "\n",
    "<span style=\"color:red;font-size:16px\">**Fill in the #TODOs and try to stick to the provided APIs.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and cleaning raw dataset (3 pt)\n",
    "In this section, you are required to obtain the provide vocabulary and load and clear the raw SNLI dataset.  \n",
    "\n",
    "The original SNLI dataset contains about 50k/10k/10k train/dev/test sentences pairs. We also provide smaller training set that contains 0.05 of the original training set in `'snli_1.0/small_0.05_snli_train.jsonl'`. You could pick one training set depending on your computation resources. Each line of the dataset file is a data point in JSON format. The `'gold_label'` field contains the label (`'entailment', 'neutral' or 'contradiction'`) for this data point. There are some data points that don't have the gold_label (their value for the gold label field is `'-'`). We will only use those examples which have a gold label. For a detailed description of the task and the dataset file, please refer to https://nlp.stanford.edu/projects/snli/.\n",
    "\n",
    "The vocabulary we will use for this homework is in the `vocabu.txt` file. It contains 19,007 tokens and all the missing words in the snli data set should be mapped to the last token in the `vocabu.txt` file which is `'<unk-0>'`.\n",
    "\n",
    "Finish the preprocessing code in the `obtain_data()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_data(file_name='snli_1.0/small_0.05_snli_train.jsonl', vocab_file_name='vocabu.txt'):\n",
    "    json_data = []\n",
    "    \n",
    "    with open(file_name) as data_file:\n",
    "        for l in data_file:\n",
    "            json_data.append(json.loads(l))\n",
    "    \n",
    "    data = []\n",
    "    for item in json_data:\n",
    "        if item['gold_label'] != '-':\n",
    "            example = (item['sentence1'], item['sentence2'], item['gold_label'])\n",
    "            data.append(example)\n",
    "    del json_data; gc.collect()\n",
    "    stoi = []\n",
    "    with open(vocab_file_name) as voc_file:\n",
    "        for token in voc_file:\n",
    "            stoi.append(token[:-1]) # remove escape characters\n",
    "    \n",
    "    return data, stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below (according to your choice of training set) and show how many data points are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in vocabulary\n",
      "19007\n",
      "snli_1.0/small_0.05_snli_train.jsonl\n",
      "27284\n"
     ]
    }
   ],
   "source": [
    "data, stoi = obtain_data('snli_1.0/small_0.05_snli_train.jsonl')\n",
    "print('# of words in vocabulary')\n",
    "print(len(stoi))\n",
    "print('snli_1.0/small_0.05_snli_train.jsonl')\n",
    "print(len(data))\n",
    "\n",
    "#print('snli_1.0/snli_1.0_train.jsonl')\n",
    "#data, stoi = obtain_data('snli_1.0/snli_1.0_train.jsonl')\n",
    "#print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build your vocabulary (6 pt)\n",
    "In this section, we will build a vocabulary python object and load pretrained Glove Embedding for your vocabulary.   Both of them will be important throughout this assignment.  \n",
    "\n",
    "The `Vocabulary` object will have three attributes, namely `embeddings`, `itos` and `stoi`.  \n",
    "\n",
    "* `stoi` is a python list which can be used to map a word into a unique id.  \n",
    "* `itos` is a python dictionary which can be used to map a given id into the corresponding word.  \n",
    "* `embeddings` is a numpy array with shape: (size_of_vocabulary, embedding_dimension). Note: The `i`th row of your embeddings will be the vector for the word whose id is `i`.\n",
    "\n",
    "Example:\n",
    "```\n",
    "embeddings[stoi['happy']] # This will give you the corresponding vector for word 'happy'.\n",
    "itos[100] # This will give you the word whose id is 100.\n",
    "```\n",
    "\n",
    "In the cell below:\n",
    "1. Directly use `stoi` to (you loaded in the last section) initiate your `vocabulary.stoi` attribute. (0.5 pt)\n",
    "2. Build `itos` according to the `stoi`. (0.5 pt)\n",
    "3. Load Glove embedding in the function `set_word_embedding(self, embedding_file)` and initiate your embeddings using pretrain Glove embedding. For detailed description and format of Glove Embeddings, refer to https://nlp.stanford.edu/projects/glove/. (4 pt)\n",
    "4. Numericalize your data in the `process_data(self, data)` function, namely convert the sentences into a list of ids and the labels into label_ids. (Use the dictionary `label_id` in the cell below for mapping.) (1 pt)\n",
    "\n",
    "Remember that your output of `process_data(self, data)` should be a list of triple `[(token_id_list, token_id_list, int)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_id = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self, stoi):\n",
    "        self.embeddings = None\n",
    "        self.itos = dict(zip(stoi,range(len(stoi))))\n",
    "        self.itos['<unk-0>'] = len(stoi)+1\n",
    "        self.stoi = stoi\n",
    "    \n",
    "    def set_word_embedding(self, embedding_file):\n",
    "        \n",
    "        f = open('glove.840B.300d.txt','r', encoding=\"utf8\")\n",
    "        model = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            splitLine = re.split(', | |\\n',line)\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([np.float32(val) for val in splitLine[1:-1]])\n",
    "            model[word] = embedding\n",
    "        \n",
    "        self.embeddings = np.array([])\n",
    "        for item in self.stoi:\n",
    "            if item in model.keys():\n",
    "                self.embeddings = np.append(self.embeddings, model[item])\n",
    "            else:\n",
    "                self.embeddings = np.append(self.embeddings, np.array(np.random.uniform(-1,1,300), dtype = np.float32))\n",
    "\n",
    "    '''\n",
    "    def dummy_embedding(self, embed_d=300):\n",
    "        # This is only a dummy embedding using random initialization. Do not use this function in the assignment.\n",
    "        for i, w in enumerate(self.stoi):\n",
    "            self.itos[w] = i\n",
    "        vocab_size = len(self.stoi)\n",
    "        self.embeddings = np.asarray(np.random.randn(vocab_size, embed_d), dtype=np.float32)\n",
    "    '''    \n",
    "    def process_data(self, data):\n",
    "        numerialized_data = []\n",
    "        nlp = spacy.load('en')\n",
    "        tokenization_1 = list()\n",
    "        tokenization_2 = list()\n",
    "        for s1, s2, y in data:\n",
    "            doc_1 = nlp(s1)\n",
    "            doc_2 = nlp(s2)\n",
    "            tmp_list_1, tmp_list_2, n_s1, n_s2 = list(),list(),list(),list()\n",
    "            \n",
    "            for token in doc_1:\n",
    "                item = token.orth_\n",
    "                if item in itos.keys():\n",
    "                    n_s1.append(itos[item])\n",
    "                else:\n",
    "                    n_s1.append(itos['<unk-0>'])\n",
    "                    \n",
    "            for token in doc_2:\n",
    "                item = token.orth_\n",
    "                if item in itos.keys():\n",
    "                    n_s2.append(itos[item])\n",
    "                else:\n",
    "                    n_s2.append(itos['<unk-0>'])\n",
    "            \n",
    "            n_y = label_id[y]\n",
    "            \n",
    "            numerialized_data.append((n_s1, n_s2, n_y))\n",
    "            \n",
    "        return numerialized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19007, 300)\n",
      "27284\n",
      "6030\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# You can use this code to check the correctness of your code.\n",
    "\n",
    "vocab = Vocabulary(stoi=stoi)\n",
    "vocab.set_word_embedding('glove.840B.300d.txt')\n",
    "n_data = vocab.process_data(data)\n",
    "\n",
    "print(vocab.embeddings.shape)\n",
    "print(len(n_data))\n",
    "\n",
    "print(vocab.itos['hello'])\n",
    "print(vocab.stoi[6030])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Padding (5 pt)\n",
    "For a neural network model to be trained on sequential natural language data, we will need to pad each sequence into a fixed length.\n",
    "\n",
    "The problem with sequential data for neural networks is that different examples might have different lengths. So we will need to pad all the data (or the data in a batch) to a fixed length (say 50) for parallel computation.\n",
    "\n",
    "In the cell below, finish the `convert_to_numpy(data, padding_length=50)` function which can convert the data into numpy ndarray and with padding. We recommend padding value to be zero.  \n",
    "\n",
    "You can do padding in corpus level (that is padding all the example in the dataset to a fixed length which means that all the example in your dataset will have the same length) or batch level (padding the example in a batch to a fixed length, different batch might have different length)\n",
    "\n",
    "Some Neural Network framework (like tensorflow) use a more advanced batching technique call bucketing. We will not implement bucketing in this assignment but refer to https://www.tensorflow.org/tutorials/seq2seq for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padded data could be something like this:\n",
    "```\n",
    "for start, end in batch_index_gen(3, len(y)):\n",
    "    print(s1[start:end])\n",
    "    print(s2[start:end])\n",
    "    print(y[start:end])\n",
    "    break\n",
    "```\n",
    "Output:\n",
    "```\n",
    "[[   5   26    7   99   10  281    6    8  293   13    4   39  388    3    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [  61  147    9    7  282  104 1797   19    4  242    3    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [   5    9   11    4   16  586    8   45    6   46   13    4 1043   11 8421  242    3    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
    "[[  15   26    7   22  593  480    3    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [   5    9    6    4  242    7  164   40   23  682   18 2635    3    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [5734    4  130   13 5079   17   52   20  152  234    8   45    3    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
    "[1 1 0]\n",
    "```\n",
    "\n",
    "You don't have to have the same output. All you will need to do is to convert your data into the format such that it can be feeded into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_index_gen(batch_size, size):\n",
    "    batch_indexer = []\n",
    "    start = 0\n",
    "    while start < size:\n",
    "        end = start + batch_size\n",
    "        if end > size:\n",
    "            end = size\n",
    "        batch_indexer.append((start, end))\n",
    "        start = end\n",
    "    return batch_indexer\n",
    "\n",
    "def convert_to_numpy(data, padding_length=50):\n",
    " \n",
    "    \n",
    "    total_size = len(data)\n",
    "    new_data = []\n",
    "    for i, (s1, s2, y) in enumerate(data):\n",
    "        if len(s1) <= padding_length:\n",
    "            s1 =  s1 + [0] *(50-len(s1))\n",
    "        else:\n",
    "            s1 = s1[:padding_length]\n",
    "        \n",
    "        if len(s2) <= padding_length:\n",
    "            s2 =  s2 + [0] *(50-len(s2))\n",
    "        else:\n",
    "            ss = ss[:padding_length]\n",
    "        \n",
    "        new_data.append((s1,s2,y))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_data = convert_to_numpy(n_data, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN Sequence-to-label Model (15 pt + 5 bonus)\n",
    "It's time to build your RNN sequence-to-label model.\n",
    "\n",
    "Remember that the input of the model is two sequences and you will use the label to train your model.  \n",
    "**RNN component is mandatory** for the model in this assignment.\n",
    "\n",
    "Recommended reading: \n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* https://discuss.pytorch.org/t/batch-processing-with-variable-length-sequences/3150 (variable length handling in pytorch)\n",
    "* https://danijar.com/variable-sequence-lengths-in-tensorflow/ (variable length handling in tensorflow)\n",
    "\n",
    "Bonus will be given to those who correctly handle variable length inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_util\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "class YourModel(nn.Module):\n",
    "    # TODO\n",
    "    #   Build your model in this cell.\n",
    "    \n",
    "    def __init__(self, h_size=128, v_size=10, d=300, mlp_d=256):\n",
    "        super(YourModel, self).__init__()\n",
    "#         self.embedding\n",
    "#         self.lstm\n",
    "        \n",
    "    def display(self):\n",
    "        for param in self.parameters():\n",
    "            print(param.data.size())\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing your model, run the code in the cell below to show your model.  \n",
    "Here is a sample output: (You don't need to have the same output)\n",
    "```\n",
    "embedding.weight torch.Size([19007, 300])\n",
    "lstm.weight_ih_l0 torch.Size([512, 300])\n",
    "lstm.weight_hh_l0 torch.Size([512, 128])\n",
    "lstm.bias_ih_l0 torch.Size([512])\n",
    "lstm.bias_hh_l0 torch.Size([512])\n",
    "lstm.weight_ih_l0_reverse torch.Size([512, 300])\n",
    "lstm.weight_hh_l0_reverse torch.Size([512, 128])\n",
    "lstm.bias_ih_l0_reverse torch.Size([512])\n",
    "lstm.bias_hh_l0_reverse torch.Size([512])\n",
    "mlp_1.weight torch.Size([256, 1024])\n",
    "mlp_1.bias torch.Size([256])\n",
    "sm.weight torch.Size([3, 256])\n",
    "sm.bias torch.Size([3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = YourModel()\n",
    "model.embedding.weight.data = torch.from_numpy(vocab.embeddings)\n",
    "model.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training your model (3 pt)\n",
    "You could now train your model batch by batch using whatever optimizer you want.  \n",
    "In order to keep track of your training, you should also print out the loss every 1000*`X` batch.  \n",
    "\n",
    "Write your code in the cell below. Print out the loss every 1000*`X` batch and your final average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation (8 pt)\n",
    "\n",
    "Complete the `eval_model(model, mode='dev')` function in the cell below for evaluate your model on dev and test set. The return value of this function should be the accuracy. Try to tune your model on the dev set and finally evaluate the model with best-dev-result on the test set and report the final test set result. **Note: You should try your model on test set only once.**\n",
    "\n",
    "If you are not satisfied with the result from the evaluation (in most cases), you could try to make some changes in your model and re-try.\n",
    "\n",
    "If you are running most things correctly, your result should easily be at least 60% on the dev set if you are using 0.05 of the SNLI training data;  and at least 80% on the dev set if you are using full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(model, mode='dev'):\n",
    "    file_name = 'snli_1.0/snli_1.0_dev.jsonl' if mode == 'dev' else 'snli_1.0/snli_1.0_test.jsonl'\n",
    "        \n",
    "    dev_data, _ = obtain_data(file_name)\n",
    "    dev_n_data = vocab.process_data(data)\n",
    "    \n",
    "    `<your dev data>` = convert_to_numpy(dev_n_data)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total = 0\n",
    "    hit = 0\n",
    "    \n",
    "    # TODO:\n",
    "    #   write your code here to show the result of your model on \n",
    "    \n",
    "    return hit / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional analysis (5 pt)\n",
    "This course is designed to train you as an NLP researcher. A researcher should not only be able to implement newly emerged models and algorithms and get them to work but also give reasons and intuitions behind every decision you make during your research (e.g. parameter and structure design).  \n",
    "In this section, write down anything you think that is important in this homework.  \n",
    "It could be:\n",
    "* The problems you encountered during the implementation and how you resolve it.\n",
    "* You were not satisfied with the results and you made some changes to (or it fails to) improve it. Why do you think those changes can (might) be helpful?  \n",
    "\n",
    "Use your imagination and try to record every detail of your experiments. The bonus will be given to novel and reasonable thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
