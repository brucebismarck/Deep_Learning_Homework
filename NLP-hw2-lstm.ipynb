{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 790.139: Natural Language Processing (Fall 2017): Coding Homework 2 (Sequence-to-Label Learning, Entailment Recognition)\n",
    "Created by TA Yixin Nie (Instructor: Mohit Bansal)\n",
    "\n",
    "\n",
    "# Instructions\n",
    "\n",
    "**All the instructions are present in the jupyter notebook (as shown in the class; and see the preview below).  \n",
    "Install jupyter notebook in your python environment and download the file below.**  \n",
    "https://drive.google.com/drive/folders/0B6i0pVGwapCdVHV1cnVNdWhvM1k?usp=sharing  \n",
    "  \n",
    "  \n",
    "**Use this directory as your workspace and write your code in the “hw2.ipynb” file. You could also add extra images or tables in the directory and link them into “hw2.ipynb” file but grading will only base on the “hw2.ipynb” file.**    \n",
    "\n",
    "**Make sure to name your directory as `\"<your_name>_hw2\"` and compress it to `\"<your_name>_hw2.zip\"`.   \n",
    "Email the file to <a href=\"mailto:comp790.hw@gmail.com\">comp790.hw@gmail.com</a> for submission.**    \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Preview\n",
    "The main goals for homework 2 are:\n",
    "1. Going through some setup procedures of training deep neural network for NLP.\n",
    "2. Getting familiar with Recurrent Neural Networks.\n",
    "3. Learning to handle variable length inputs in deep learning framework (Pytorch, Tensorflow).\n",
    "4. Building a sequence-to-label model for natural language inference task.\n",
    "5. Learning to improve your model by interpreting your experiment result.\n",
    "\n",
    "Notice:  \n",
    "You can test or run your code in any environment but you could only show your codes, your results and your write-ups in this single notebook file. We will not re-run your code for grading.  \n",
    "\n",
    "<span style=\"color:red;font-size:16px\">**Fill in the #TODOs and try to stick to the provided APIs.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and cleaning raw dataset (3 pt)\n",
    "In this section, you are required to obtain the provide vocabulary and load and clear the raw SNLI dataset.  \n",
    "\n",
    "The original SNLI dataset contains about 50k/10k/10k train/dev/test sentences pairs. We also provide smaller training set that contains 0.05 of the original training set in `'snli_1.0/small_0.05_snli_train.jsonl'`. You could pick one training set depending on your computation resources. Each line of the dataset file is a data point in JSON format. The `'gold_label'` field contains the label (`'entailment', 'neutral' or 'contradiction'`) for this data point. There are some data points that don't have the gold_label (their value for the gold label field is `'-'`). We will only use those examples which have a gold label. For a detailed description of the task and the dataset file, please refer to https://nlp.stanford.edu/projects/snli/.\n",
    "\n",
    "The vocabulary we will use for this homework is in the `vocabu.txt` file. It contains 19,007 tokens and all the missing words in the snli data set should be mapped to the last token in the `vocabu.txt` file which is `'<unk-0>'`.\n",
    "\n",
    "Finish the preprocessing code in the `obtain_data()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_data(file_name='snli_1.0/small_0.05_snli_train.jsonl', vocab_file_name='vocabu.txt'):\n",
    "    json_data = []\n",
    "    \n",
    "    with open(file_name) as data_file:\n",
    "        for l in data_file:\n",
    "            json_data.append(json.loads(l))\n",
    "    \n",
    "    data = []\n",
    "    for item in json_data:\n",
    "        if item['gold_label'] != '-':\n",
    "            example = (item['sentence1'], item['sentence2'], item['gold_label'])\n",
    "            data.append(example)\n",
    "    \n",
    "    stoi = []\n",
    "    with open(vocab_file_name) as voc_file:\n",
    "        for token in voc_file:\n",
    "            stoi.append(token[:-1]) # remove escape characters\n",
    "    \n",
    "    return data, stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below (according to your choice of training set) and show how many data points are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in vocabulary\n",
      "19007\n",
      "snli_1.0/small_0.05_snli_train.jsonl\n",
      "27284\n"
     ]
    }
   ],
   "source": [
    "data, stoi = obtain_data('snli_1.0/small_0.05_snli_train.jsonl')\n",
    "print('# of words in vocabulary')\n",
    "print(len(stoi))\n",
    "print('snli_1.0/small_0.05_snli_train.jsonl')\n",
    "print(len(data))\n",
    "\n",
    "#print('snli_1.0/snli_1.0_train.jsonl')\n",
    "#data, stoi = obtain_data('snli_1.0/snli_1.0_train.jsonl')\n",
    "#print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build your vocabulary (6 pt)\n",
    "In this section, we will build a vocabulary python object and load pretrained Glove Embedding for your vocabulary.   Both of them will be important throughout this assignment.  \n",
    "\n",
    "The `Vocabulary` object will have three attributes, namely `embeddings`, `itos` and `stoi`.  \n",
    "\n",
    "* `stoi` is a python list which can be used to map a word into a unique id.  \n",
    "* `itos` is a python dictionary which can be used to map a given id into the corresponding word.  \n",
    "* `embeddings` is a numpy array with shape: (size_of_vocabulary, embedding_dimension). Note: The `i`th row of your embeddings will be the vector for the word whose id is `i`.\n",
    "\n",
    "Example:\n",
    "```\n",
    "embeddings[stoi['happy']] # This will give you the corresponding vector for word 'happy'.\n",
    "itos[100] # This will give you the word whose id is 100.\n",
    "```\n",
    "\n",
    "In the cell below:\n",
    "1. Directly use `stoi` to (you loaded in the last section) initiate your `vocabulary.stoi` attribute. (0.5 pt)\n",
    "2. Build `itos` according to the `stoi`. (0.5 pt)\n",
    "3. Load Glove embedding in the function `set_word_embedding(self, embedding_file)` and initiate your embeddings using pretrain Glove embedding. For detailed description and format of Glove Embeddings, refer to https://nlp.stanford.edu/projects/glove/. (4 pt)\n",
    "4. Numericalize your data in the `process_data(self, data)` function, namely convert the sentences into a list of ids and the labels into label_ids. (Use the dictionary `label_id` in the cell below for mapping.) (1 pt)\n",
    "\n",
    "Remember that your output of `process_data(self, data)` should be a list of triple `[(token_id_list, token_id_list, int)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_id = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self, stoi):\n",
    "        self.embeddings = None\n",
    "        self.itos = dict(zip(stoi,range(len(stoi))))\n",
    "        self.stoi = stoi\n",
    "    \n",
    "    def set_word_embedding(self, embedding_file):\n",
    "        \n",
    "        f = open('glove.840B.300d.txt','r', encoding=\"utf8\")\n",
    "        model = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            splitLine = re.split(', | |\\n',line)\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([np.float32(val) for val in splitLine[1:-1]])\n",
    "            model[word] = embedding\n",
    "        \n",
    "        self.embeddings = np.array([])\n",
    "        for item in self.stoi:\n",
    "            if item in model.keys():\n",
    "                self.embeddings = np.append(self.embeddings, [[model[item]]])\n",
    "            else:\n",
    "                self.embeddings = np.append(self.embeddings, \\\n",
    "                                [[np.array(np.random.uniform(-1,1,300), dtype = np.float32)]])\n",
    "\n",
    "    '''\n",
    "    def dummy_embedding(self, embed_d=300):\n",
    "        # This is only a dummy embedding using random initialization. Do not use this function in the assignment.\n",
    "        for i, w in enumerate(self.stoi):\n",
    "            self.itos[w] = i\n",
    "        vocab_size = len(self.stoi)\n",
    "        self.embeddings = np.asarray(np.random.randn(vocab_size, embed_d), dtype=np.float32)\n",
    "    '''    \n",
    "    def process_data(self, data):\n",
    "        numerialized_data = []\n",
    "        nlp = spacy.load('en')\n",
    "        tokenization_1 = list()\n",
    "        tokenization_2 = list()\n",
    "        for s1, s2, y in data:\n",
    "            doc_1 = nlp(s1)\n",
    "            doc_2 = nlp(s2)\n",
    "            tmp_list_1, tmp_list_2, n_s1, n_s2 = list(),list(),list(),list()\n",
    "            \n",
    "            for token in doc_1:\n",
    "                item = token.orth_\n",
    "                if item in self.itos.keys():\n",
    "                    n_s1.append(self.itos[item])\n",
    "                else:\n",
    "                    n_s1.append(self.itos[self.stoi[-1]])\n",
    "                    \n",
    "            for token in doc_2:\n",
    "                item = token.orth_\n",
    "                if item in self.itos.keys():\n",
    "                    n_s2.append(self.itos[item])\n",
    "                else:\n",
    "                    n_s2.append(self.itos[self.stoi[-1]])\n",
    "            \n",
    "            n_y = label_id[y]\n",
    "            \n",
    "            numerialized_data.append((n_s1, n_s2, n_y))\n",
    "            \n",
    "        return numerialized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5702100,)\n",
      "27284\n",
      "6030\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# You can use this code to check the correctness of your code.\n",
    "\n",
    "vocab = Vocabulary(stoi=stoi)\n",
    "vocab.set_word_embedding('glove.840B.300d.txt')\n",
    "n_data = vocab.process_data(data)\n",
    "\n",
    "print(vocab.embeddings.shape)\n",
    "print(len(n_data))\n",
    "\n",
    "print(vocab.itos['hello'])\n",
    "print(vocab.stoi[6030])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19007, 300)\n"
     ]
    }
   ],
   "source": [
    "vocab.embeddings = vocab.embeddings.reshape((19007,300))\n",
    "print(vocab.embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Padding (5 pt)\n",
    "For a neural network model to be trained on sequential natural language data, we will need to pad each sequence into a fixed length.\n",
    "\n",
    "The problem with sequential data for neural networks is that different examples might have different lengths. So we will need to pad all the data (or the data in a batch) to a fixed length (say 50) for parallel computation.\n",
    "\n",
    "In the cell below, finish the `convert_to_numpy(data, padding_length=50)` function which can convert the data into numpy ndarray and with padding. We recommend padding value to be zero.  \n",
    "\n",
    "You can do padding in corpus level (that is padding all the example in the dataset to a fixed length which means that all the example in your dataset will have the same length) or batch level (padding the example in a batch to a fixed length, different batch might have different length)\n",
    "\n",
    "Some Neural Network framework (like tensorflow) use a more advanced batching technique call bucketing. We will not implement bucketing in this assignment but refer to https://www.tensorflow.org/tutorials/seq2seq for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padded data could be something like this:\n",
    "```\n",
    "for start, end in batch_index_gen(3, len(y)):\n",
    "    print(s1[start:end])\n",
    "    print(s2[start:end])\n",
    "    print(y[start:end])\n",
    "    break\n",
    "```\n",
    "Output:\n",
    "```\n",
    "[[   5   26    7   99   10  281    6    8  293   13    4   39  388    3    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [  61  147    9    7  282  104 1797   19    4  242    3    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [   5    9   11    4   16  586    8   45    6   46   13    4 1043   11 8421  242    3    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
    "[[  15   26    7   22  593  480    3    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [   5    9    6    4  242    7  164   40   23  682   18 2635    3    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
    " [5734    4  130   13 5079   17   52   20  152  234    8   45    3    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
    "[1 1 0]\n",
    "```\n",
    "\n",
    "You don't have to have the same output. All you will need to do is to convert your data into the format such that it can be feeded into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_index_gen(batch_size, size):\n",
    "    batch_indexer = []\n",
    "    start = 0\n",
    "    while start < size:\n",
    "        end = start + batch_size\n",
    "        if end > size:\n",
    "            end = size\n",
    "        batch_indexer.append((start, end))\n",
    "        start = end\n",
    "    return batch_indexer\n",
    "\n",
    "def convert_to_numpy(data, padding_length=50):\n",
    "    total_size = len(data)\n",
    "    s1_ = []; s2_ = []; y_ = []\n",
    "    for i, (s1, s2, y) in enumerate(data):\n",
    "        if len(s1) <= padding_length:\n",
    "            s1 =  s1 + [0] *(50-len(s1))\n",
    "        else:\n",
    "            s1 = s1[:padding_length]\n",
    "        \n",
    "        if len(s2) <= padding_length:\n",
    "            s2 =  s2 + [0] *(50-len(s2))\n",
    "        else:\n",
    "            s2 = s2[:padding_length]\n",
    "        \n",
    "        s1_.append(s1)\n",
    "        s2_.append(s2)\n",
    "        y_.append(y)\n",
    "    return s1_, s2_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1, s2, y = convert_to_numpy(n_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 26, 7, 99, 10, 281, 6, 8, 293, 13, 4, 39, 388, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [61, 147, 9, 7, 282, 104, 1797, 19, 4, 242, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 9, 11, 4, 16, 586, 8, 45, 6, 46, 13, 4, 1043, 11, 8421, 242, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[15, 26, 7, 22, 593, 480, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 9, 6, 4, 242, 7, 164, 40, 23, 682, 18, 2635, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5734, 4, 130, 13, 5079, 17, 52, 20, 152, 234, 8, 45, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "for start, end in batch_index_gen(3, len(y)):\n",
    "    print(s1[start:end])\n",
    "    print(s2[start:end])\n",
    "    print(y[start:end])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN Sequence-to-label Model (15 pt + 5 bonus)\n",
    "It's time to build your RNN sequence-to-label model.\n",
    "\n",
    "Remember that the input of the model is two sequences and you will use the label to train your model.  \n",
    "**RNN component is mandatory** for the model in this assignment.\n",
    "\n",
    "Recommended reading: \n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* https://discuss.pytorch.org/t/batch-processing-with-variable-length-sequences/3150 (variable length handling in pytorch)\n",
    "* https://danijar.com/variable-sequence-lengths-in-tensorflow/ (variable length handling in tensorflow)\n",
    "\n",
    "Bonus will be given to those who correctly handle variable length inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # inputs of lstm : input, (h_0, c_0)\n",
    "        #   input: (seq_len, batch, input_size)\n",
    "        #   h_0 : number_layers* num_directions, batch, hidden_size \n",
    "        #         here we have one layer and one direction. \n",
    "        \n",
    "        #  c_0 : numb_layers*num_directions, batch, hidden_size\n",
    "        \n",
    "        # output of lstm :\n",
    "        #   h_n and c_n : same to the h_0 and c_0\n",
    "        #   lstm_out : (seq_len, batch, hidden_size*num_dir)\n",
    "        \n",
    "                # 根据lstm的输入要求， 第一个其实是h0的size 第二个是c0的size\n",
    "        #>>> rnn = nn.LSTM(10, 20, 2)   # 2 代表了要么是两层！ \n",
    "                            # 两个10分别代表 input_size, hidden_size\n",
    "        #>>> input = Variable(torch.randn(5, 3, 10)) # 5,3,10 代表了 seq_len = 5, \n",
    "                                                     # batch_size = 3, input_size(句长)= 10\n",
    "        #>>> h0 = Variable(torch.randn(2, 3, 20))\n",
    "        #>>> c0 = Variable(torch.randn(2, 3, 20))\n",
    "        #>>> output, hn = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_util\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    # TODO\n",
    "    #   Build your model in this cell.\n",
    "    \n",
    "    def __init__(self, h_size=128, d=300, label_size = 3, v_size = 10, batch_size = 20):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = h_size\n",
    "        self.embedding_dim = d\n",
    "        self.label_size = label_size\n",
    "        self.v_size = v_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(v_size, d)\n",
    "        self.lstm = nn.LSTM(input_size= d, hidden_size= h_size, num_layers= 1)\n",
    "        self.hidden2tag = nn.Linear(h_size*100, h_size*100)\n",
    "        self.linear2 = nn.Linear(h_size*100, label_size)\n",
    "        \n",
    "    def display(self):\n",
    "        for param in self.parameters():\n",
    "            #print(param)\n",
    "            print(param.data.size())\n",
    "            \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda(),#那个1 代表了layer×dim,\n",
    "                Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda())#.cuda())\n",
    "\n",
    "    def forward(self, sentence1_var, sentence2_var, hidden1, hidden2):\n",
    "        embeds1 = self.embedding(sentence1_var).type_as(torch.FloatTensor()).cuda()#.cuda()\n",
    "        embeds2 = self.embedding(sentence2_var).type_as(torch.FloatTensor()).cuda()#.cuda()\n",
    "        \n",
    "        lstm1_out, hidden1 = self.lstm(embeds1.view(len(sentence1_var[0]),self.batch_size,self.embedding_dim), hidden1)\n",
    "        lstm2_out, hidden2 = self.lstm(embeds2.view(len(sentence2_var[0]),self.batch_size,self.embedding_dim), hidden2)\n",
    "        \n",
    "        \n",
    "        combined  = torch.cat((lstm1_out.view(20,-1), lstm2_out.view(20,-1)),1).cuda()\n",
    "        tag_space = F.sigmoid(self.hidden2tag(combined)).cuda()\n",
    "        tag_space2 = F.sigmoid(self.linear2(tag_space)).cuda()#.cuda()\n",
    "        \n",
    "        probs = F.softmax(tag_space).cuda()#.cuda()\n",
    "        \n",
    "\n",
    "        return probs, hidden1, hidden2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used is 1511144257\n",
      "Total loss is \n",
      " 9.4572\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Time used is 13\n",
      "Total loss is \n",
      " 945.7078\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Time used is 13\n",
      "Total loss is \n",
      " 945.7076\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Time used is 13\n",
      "Total loss is \n",
      " 945.7076\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Time used is 13\n",
      "Total loss is \n",
      " 945.7076\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Time used is 13\n",
      "Total loss is \n",
      " 945.7076\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Time used is 13\n",
      "Total loss is \n",
      " 945.7076\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Time used is 13\n",
      "Total loss is \n",
      " 945.7076\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f45595ccd05b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0ms1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_shuffle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0ms2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2_shuffle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0my_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_shuffle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device_id, async)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCudaTransfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, device_id, async)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "model = LSTMModel(batch_size= batch_size)\n",
    "model.embedding.weight.data = torch.from_numpy(vocab.embeddings)\n",
    "model.cuda()#.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "loss_function.cuda()#.cuda()\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "hidden1 = model.init_hidden()\n",
    "hidden2 = model.init_hidden()\n",
    "\n",
    "for epoch in range(20):\n",
    "    s1_shuffle = []\n",
    "    s2_shuffle = []\n",
    "    y_shuffle = []\n",
    "    index_shuf = list(range(len(s1)))\n",
    "    shuffle(index_shuf)\n",
    "    for i in index_shuf:\n",
    "        s1_shuffle.append(s1[i])\n",
    "        s2_shuffle.append(s2[i])\n",
    "        y_shuffle.append(y[i])\n",
    "    \n",
    "    \n",
    "    for i in range(len(y)//batch_size):\n",
    "        s1_ = Variable(torch.LongTensor(s1_shuffle[i*batch_size: (i+1)*batch_size])).cuda()\n",
    "        s2_ = Variable(torch.LongTensor(s2_shuffle[i*batch_size: (i+1)*batch_size])).cuda()\n",
    "        y_  = Variable(torch.LongTensor(y_shuffle[i*batch_size: (i+1)*batch_size])).cuda()\n",
    "    \n",
    "        hidden1 = repackage_hidden(hidden1)\n",
    "        hidden2 = repackage_hidden(hidden2)\n",
    "        \n",
    "        model.zero_grad()\n",
    "    \n",
    "        probs, hidden1, hidden2 = model(s1_, s2_, hidden1, hidden2)\n",
    "        #probs = model(s1_, s2_)\n",
    "        \n",
    "        loss = loss_function(probs, y_)\n",
    "        \n",
    "        loss.backward()\n",
    "        #loss.backward()  # 这个地方之前为啥要retain 是因为每次的hidden都是一个zero tensor 没办法backward（）\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "        if i%100 == 0:\n",
    "            print('Time used is %d' %(time.time() - start))\n",
    "            print('Total loss is %s' %total_loss)\n",
    "            total_loss = 0\n",
    "            start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.4698  0.4955  0.5253\n",
      " 0.5064  0.4754  0.4631\n",
      " 0.4779  0.4982  0.4813\n",
      " 0.4841  0.5005  0.4870\n",
      " 0.4881  0.4904  0.4702\n",
      " 0.5083  0.4725  0.4922\n",
      " 0.5060  0.4988  0.4846\n",
      " 0.4783  0.4957  0.4621\n",
      " 0.5112  0.4621  0.4710\n",
      " 0.5074  0.4944  0.4734\n",
      " 0.5128  0.4784  0.4625\n",
      " 0.4702  0.4998  0.4809\n",
      " 0.5100  0.5079  0.4870\n",
      " 0.5339  0.5171  0.5323\n",
      " 0.5018  0.4885  0.4747\n",
      " 0.4720  0.5224  0.4772\n",
      " 0.4813  0.4844  0.4600\n",
      " 0.4767  0.4835  0.4610\n",
      " 0.4843  0.4899  0.4492\n",
      " 0.5024  0.4847  0.4581\n",
      "[torch.FloatTensor of size 20x3]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = LSTMModel()\\nmodel.embedding.weight.data = torch.from_numpy(vocab.embeddings)\\nmodel.display()\\nprint(model)'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = False\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        if use_cuda:\n",
    "            return Variable(h.data).cuda()\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "embedding_layer = nn.Embedding(19007, 300)\n",
    "batch_size = 20\n",
    "input = Variable(torch.LongTensor(s1[0:batch_size])) #原来是2*50*300 用了view改成 50*2*300\n",
    "intermediate = embedding_layer(input).view(len(s1[0]),batch_size,300)\n",
    "hidden = (Variable(torch.zeros(1, batch_size, 128)), Variable(torch.zeros(1, batch_size, 128)))\n",
    "lstm = nn.LSTM(300,128,1)\n",
    "lstm_out, hidden_ = lstm(intermediate, hidden)\n",
    "#print(lstm_out.view(-1,)\n",
    "\n",
    "intermediate2 = torch.cat((lstm_out.view(20,-1), lstm_out.view(20,-1)),1)# 2*256\n",
    "#print(intermediate2)\n",
    "hidden2tag = nn.Linear(128*100, 3)\n",
    "a = F.sigmoid(hidden2tag(intermediate2))\n",
    "print(a)\n",
    "#hidden_ =  repackage_hidden(hidden_)\n",
    "\n",
    "#lstm_out, hidden_ = lstm(intermediate, hidden_)\n",
    "'''\n",
    "model = LSTMModel()\n",
    "model.embedding.weight.data = torch.from_numpy(vocab.embeddings)\n",
    "model.display()\n",
    "print(model)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training your model (3 pt)\n",
    "You could now train your model batch by batch using whatever optimizer you want.  \n",
    "In order to keep track of your training, you should also print out the loss every 1000*`X` batch.  \n",
    "\n",
    "Write your code in the cell below. Print out the loss every 1000*`X` batch and your final average loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation (8 pt)\n",
    "\n",
    "Complete the `eval_model(model, mode='dev')` function in the cell below for evaluate your model on dev and test set. The return value of this function should be the accuracy. Try to tune your model on the dev set and finally evaluate the model with best-dev-result on the test set and report the final test set result. **Note: You should try your model on test set only once.**\n",
    "\n",
    "If you are not satisfied with the result from the evaluation (in most cases), you could try to make some changes in your model and re-try.\n",
    "\n",
    "If you are running most things correctly, your result should easily be at least 60% on the dev set if you are using 0.05 of the SNLI training data;  and at least 80% on the dev set if you are using full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(model, mode='dev'):\n",
    "    file_name = 'snli_1.0/snli_1.0_dev.jsonl' if mode == 'dev' else 'snli_1.0/snli_1.0_test.jsonl'\n",
    "        \n",
    "    dev_data, _ = obtain_data(file_name)\n",
    "    dev_n_data = vocab.process_data(data)\n",
    "    \n",
    "    #`<your dev data>` = convert_to_numpy(dev_n_data)\n",
    "    \n",
    "    model.eval()\n",
    "    new_dev_data = convert_to_numpy(vocab.process_data(dev_data), 50)\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    y = []\n",
    "    for item in new_data:\n",
    "        s1.append(item[0])\n",
    "        s2.append(item[1])\n",
    "        y.append(item[2])\n",
    "\n",
    "    y = [item for item in y]\n",
    "    \n",
    "    pred = np.argmax(model(autograd.Variable(s1), autograd.Variable(s2)).data.numpy(),axis = 1)\n",
    "\n",
    "    \n",
    "    total = 0\n",
    "    hit = 0\n",
    "    \n",
    "    # TODO:\n",
    "    #   write your code here to show the result of your model on \n",
    "    for i in range(len(label)):\n",
    "        if label[i] == pred[i]:\n",
    "            hit = hit + 1\n",
    "        total = total + 1\n",
    "    \n",
    "    return hit / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I can't train the model well. Will figure out in the weekend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional analysis (5 pt)\n",
    "This course is designed to train you as an NLP researcher. A researcher should not only be able to implement newly emerged models and algorithms and get them to work but also give reasons and intuitions behind every decision you make during your research (e.g. parameter and structure design).  \n",
    "In this section, write down anything you think that is important in this homework.  \n",
    "It could be:\n",
    "* The problems you encountered during the implementation and how you resolve it.\n",
    "* You were not satisfied with the results and you made some changes to (or it fails to) improve it. Why do you think those changes can (might) be helpful?  \n",
    "\n",
    "Use your imagination and try to record every detail of your experiments. The bonus will be given to novel and reasonable thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. I moved from win to linux for Pytorch. One thing is the GPU memory control for pytorch.\n",
    "   I am not sure why, but I am asked to add 'retain_graph = True' in loss.backward, which incrase GPU memory \n",
    "   a lot and does not clear GPU memory when it is not used.\n",
    "   \n",
    "   So after 500 batch, OOM occured. I moved back to CPU.\n",
    "   \n",
    "2. Not very sure how to handle variable length. Or not sure why we need to pack the sentence and unpack them.\n",
    "    Or the advantage to do that. So I used the general method.\n",
    "    \n",
    "3. Training speed. The training speed on my desktop is super slow. I don't understand why, but will keep working on \n",
    "    it this weekend.\n",
    "    \n",
    "4. I think we should do Bi directional LSTM. It is obvious some words after the target also provides information. \n",
    "    Using bi directional LSTM may help.\n",
    "\n",
    "5. One thing I faced is I want to append <unk-0> when the model faced some words not in glove dictionary. Hoever, \n",
    "    that will introduce word with index 19008 which is out of range. I used long time to figure out the problem of\n",
    "    this error.\n",
    "\n",
    "6. Most of my time in these two weeks was spent on interview (I will graduate after this semester). I don't have \n",
    "    enough time to modify my model and code.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
